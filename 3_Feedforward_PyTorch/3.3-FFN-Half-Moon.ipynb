{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose and goals\n",
    "In this notebook you will implement a simple neural network in PyTorch.\n",
    "\n",
    "The building blocks of PyTorch are Tensors, and Operations, with these we can form dynamic computational graphs that represent neural networks.\n",
    "In this exercise we'll start right away by defining a logistic regression model using these simple building blocks.\n",
    "We'll initially start with a simple 2D and binary (i.e. two-class) classification problem where the class decision boundary can be visualized.\n",
    "Initially we show that logistic regression can only separate classes linearly.\n",
    "Adding a nonlinear hidden layer to the algorithm permits nonlinear class separation.\n",
    "\n",
    "In this notebook you should:\n",
    "* **First** run the code as is, and see what it does.\n",
    "* **Then** modify the code, following the instructions in the bottom of the notebook.\n",
    "* **Lastly** play around a bit, and do some small experiments that you come up with.\n",
    "\n",
    "> We assume that you are already familiar with backpropagation (if not please see [Andrej Karpathy](http://cs.stanford.edu/people/karpathy/) or [Michal Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and supporting functions\n",
    "Load dependencies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "\n",
    "# Do not worry about the code below for now, it is used for plotting later\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    #from https://github.com/dennybritz/nn-from-scratch/blob/master/nn-from-scratch.ipynb\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    \n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    yy = yy.astype('float32')\n",
    "    xx = xx.astype('float32')\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])[:,0]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=-y, cmap=plt.cm.Spectral)\n",
    "\n",
    "def onehot(t, num_classes):\n",
    "    out = np.zeros((t.shape[0], num_classes))\n",
    "    for row, col in enumerate(t):\n",
    "        out[row, col] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem \n",
    "We'll initally demonstrate that Multi-layer Perceptrons (MLPs) can classify nonlinear problems, whereas a simple logistic regression model cannot.\n",
    "For ease of visualization and computational speed we initially experiment on the simple 2D half-moon dataset, visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "num_samples = 300\n",
    "\n",
    "X, y = sklearn.datasets.make_moons(num_samples, noise=0.20)\n",
    "\n",
    "# define train, validation, and test sets\n",
    "X_tr = X[:100].astype('float32')\n",
    "X_val = X[100:200].astype('float32')\n",
    "X_te = X[200:].astype('float32')\n",
    "\n",
    "# and labels\n",
    "y_tr = y[:100].astype('int32')\n",
    "y_val = y[100:200].astype('int32')\n",
    "y_te = y[200:].astype('int32')\n",
    "\n",
    "plt.scatter(X_tr[:,0], X_tr[:,1], s=40, c=y_tr, cmap=plt.cm.Spectral)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "num_features = X_tr.shape[-1]\n",
    "num_output = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Logistic Regression to \"Deep Learning\"\n",
    "The code implements logistic regression. In section [__Assignments Half Moon__](#Assignments-Half-Moon) (bottom of this notebook) you are asked to modify the code into a neural network.\n",
    "\n",
    "The standard building block for neural networks are layers, the simplest of which is called a *fully-connected layer* or *dense feed forward layer*, and it is computed as follows:\n",
    "\n",
    "$$y = g(W^{\\top} x + b)$$\n",
    "\n",
    "where $x$ is the input vector, $y$ is the output vector, $W, b$ are the weights and biases (a matrix and vector respectively) and $g$ is a non-linear function, called *activation function*.\n",
    "The *dense* part of the name comes from the fact that every element of $x$ contributes to every element of $y$.\n",
    "And the *feed forward* part of the name means that the layer processes each input independently. \n",
    "If we were to draw the layer it would be acyclical.\n",
    "Later we will see layers that break from both of these conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x$ has shape `[batch_size, num_features]`,\n",
    "- $W$ has shape `[num_units, num_features]`,\n",
    "- $b$ has `[num_units]`, and\n",
    "- $y$ has then `[batch_size, num_units]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch 101\n",
    "\n",
    "In this first exercise we will use basic PyTorch functions so that you can learn how to build it from scratch. This will help you later if you want to build your own custom operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Parameters`](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter) have a very special property when used with [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=module#torch.nn.Module)s - when they’re assigned as `Module` attributes they are automatically added to the list of its parameters, and will appear e.g. in the `parameters()` iterator. \\\n",
    "Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state (more on this later) in the model. If there was no such class as `Parameter`, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Setting up variables, these variables are weights in your \n",
    "        # network that can be updated while running our graph.\n",
    "        # Notice, to make a hidden layer, the weights need to have the \n",
    "        # following dimensionality:\n",
    "        #   W[number_of_units_going_out, number_of_units_going_in]\n",
    "        #   b[number_of_units_going_out]\n",
    "        # in the example below we have 2 input units (num_features) and 2 output units (num_output)\n",
    "        # so our weights become W[2, 2], b[2]\n",
    "        # if we want to make a hidden layer with 100 units, we need to define the shape of the\n",
    "        # first weight to W[100, 2], b[100] and the shape of the second weight to W[2, 100], b[2]\n",
    "        \n",
    "        # first layer\n",
    "        self.W_1 = nn.Parameter(torch.randn(num_output, num_features)) \n",
    "        self.b_1 = nn.Parameter(torch.randn(num_output))\n",
    "        \n",
    "        # second layer (to be completed as an exercise)\n",
    "        # NB when you create a second layer, remember that you also must change parts of the first layer\n",
    "        # self.W_2 = <YOUR CODE HERE>\n",
    "        # self.b_2 = <YOUR CODE HERE>\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Setting up ops, these ops will define edges along our computational graph\n",
    "        # The below ops will compute a logistic regression, \n",
    "        # but can be modified to compute a neural network\n",
    "        x = F.linear(x, self.W_1, self.b_1)\n",
    "        \n",
    "        # second layer (to be completed as an exercise)\n",
    "        # NB when you create a second layer, remember that you also must change parts of the first layer\n",
    "        # x = F.linear(x, self.W_2, self.b_2)\n",
    "        return F.softmax(x, dim=1) # softmax to be performed on the second dimension\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing how to print your tensors is useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all parameters in your network\n",
    "print(\"NAMED PARAMETERS\")\n",
    "print(list(net.named_parameters()))\n",
    "print()\n",
    "# the .parameters() method simply gives the Tensors in the list\n",
    "print(\"PARAMETERS\")\n",
    "print(list(net.parameters()))\n",
    "print()\n",
    "\n",
    "# list individual parameters by name\n",
    "print('WEIGHTS')\n",
    "print(net.W_1)\n",
    "print(net.W_1.size())\n",
    "print('\\nBIAS')\n",
    "print(net.b_1)\n",
    "print(net.b_1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Parameter\n",
    "\n",
    "Ok, let's investigate what a Parameter is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = net.W_1\n",
    "print(\"## this is the tensor\")\n",
    "print(param.data)\n",
    "print(\"\\n## this is the tensor's gradient\")\n",
    "print(param.grad)\n",
    "# notice, the gradient is undefined because we have not yet run a backward pass\n",
    "\n",
    "print(\"\\n## is it a leaf in the graph?\")\n",
    "print(param.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excluding subgraphs from backward propagation\n",
    "\n",
    "To exclude part of your computational graph (i.e. a subgraph) from backward propagation, simply set the relevant tensors' attribute `requires_grad` to `False`.\n",
    "\n",
    "If there’s a single input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it. Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test network\n",
    "\n",
    "To use our network we can simply call our graph, and it will dynamically be created. Here is an example of running the network's forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(5, num_features)\n",
    "# the net.__call__ runs some pre-defined functions\n",
    "# both before and after running net.forward()\n",
    "# see http://pytorch.org/docs/master/_modules/torch/nn/modules/module.html\n",
    "\n",
    "print('input')\n",
    "print(X)\n",
    "\n",
    "print('\\noutput')\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Parameter`s are a special kind of `Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the gradients\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(7, num_features)\n",
    "out = net(X)\n",
    "# we need to give a tensor of gradients to .backward,\n",
    "# we give a dummy tensor\n",
    "out.backward(torch.randn(7, num_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for details on `.backward()`, see http://pytorch.org/docs/master/autograd.html#torch.autograd.backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the gradients\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, let's try and zero the accumulated gradients\n",
    "net.zero_grad()\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "Let's define a custom loss function to compute how good our graph is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(ys, ts):\n",
    "    # computing cross entropy per sample\n",
    "    cross_entropy = -torch.sum(ts * torch.log(ys), dim=1, keepdim=False)\n",
    "    # averaging over samples\n",
    "    return torch.mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our neural network we need to update the parameters in the direction of the negative gradient w.r.t the cost function we defined earlier.\n",
    "We can use [`torch.optim`](http://pytorch.org/docs/master/optim.html) to get the gradients with some update rule for all parameters in the network.\n",
    "\n",
    "Heres a small animation of gradient descent: http://imgur.com/a/Hqolp, which also illustrates which challenges optimizers might face, e.g. saddle points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we make the prediction functions, such that we can get an accuracy measure over a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], torch.max(ts, 1)[1])\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to utilize our `optimizer` repeatedly in order to optimize our weights `W_1` and `b_1` to make the best possible linear seperation of the half moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of training passses\n",
    "num_epochs = 1000\n",
    "# store loss and accuracy for information\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "def pred(X):\n",
    "    \"\"\" Compute graph's prediction and return numpy array\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "    \"\"\"\n",
    "    X = torch.from_numpy(X)\n",
    "    y = net(X)\n",
    "    return y.data.numpy()\n",
    "\n",
    "# plot boundary on testset before training session\n",
    "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
    "plt.title(\"Untrained Classifier\")\n",
    "\n",
    "# training loop\n",
    "for e in range(num_epochs):\n",
    "    # get training input and expected output as torch Variables and make sure type is correct\n",
    "    tr_input = torch.from_numpy(X_tr)\n",
    "    tr_targets = torch.from_numpy(onehot(y_tr, num_output)).float()\n",
    "    \n",
    "    # zeroize accumulated gradients in parameters\n",
    "    optimizer.zero_grad()\n",
    "    # predict by running forward pass\n",
    "    tr_output = net(tr_input)\n",
    "    # compute cross entropy loss\n",
    "    tr_loss = cross_entropy(tr_output, tr_targets)\n",
    "    # compute gradients given loss\n",
    "    tr_loss.backward()\n",
    "    # update the parameters given the computed gradients\n",
    "    optimizer.step()\n",
    "    train_acc = accuracy(tr_output, tr_targets)\n",
    "    \n",
    "    # store training loss\n",
    "    train_losses.append(tr_loss.data.numpy())\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # get validation input and expected output as torch Variables and make sure type is correct\n",
    "    val_input = torch.from_numpy(X_val)\n",
    "    val_targets = torch.from_numpy(onehot(y_val, num_output)).float()\n",
    "    \n",
    "    # predict with validation input\n",
    "    val_output = net(val_input)\n",
    "    # compute loss and accuracy\n",
    "    val_loss = cross_entropy(val_output, val_targets)\n",
    "    val_acc = accuracy(val_output, val_targets)\n",
    "    \n",
    "    # store loss and accuracy\n",
    "    val_losses.append(val_loss.data.numpy())\n",
    "    val_accs.append(val_acc.data.numpy())\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch %i, \"\n",
    "              \"Train Cost: %0.3f\"\n",
    "              \"\\tVal Cost: %0.3f\"\n",
    "              \"\\t Val acc: %0.3f\" % (e, \n",
    "                                     train_losses[-1],\n",
    "                                     val_losses[-1],\n",
    "                                     val_accs[-1]))\n",
    "        \n",
    "        \n",
    "# get test input and expected output\n",
    "te_input = torch.from_numpy(X_te)\n",
    "te_targets = torch.from_numpy(onehot(y_te, num_output)).float()\n",
    "# predict on testset\n",
    "te_output = net(te_input)\n",
    "# compute loss and accuracy\n",
    "te_loss = cross_entropy(te_output, te_targets)\n",
    "te_acc = accuracy(te_output, te_targets)\n",
    "print(\"\\nTest Cost: %0.3f\\tTest Accuracy: %0.3f\" % (te_loss.data.numpy(), te_acc.data.numpy()))\n",
    "\n",
    "# plot boundary on testset after training session\n",
    "plot_decision_boundary(lambda x: pred(x), X_te, y_te)\n",
    "plt.title(\"Trained Classifier\")\n",
    "\n",
    "plt.figure()\n",
    "epoch = np.arange(len(train_losses))\n",
    "plt.plot(epoch, train_losses, 'r', label='Train Loss')\n",
    "plt.plot(epoch, val_losses, 'b', label='Val Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_accs, 'r', label='Train Acc')\n",
    "plt.plot(epoch, val_accs, 'b', label='Val Acc')\n",
    "plt.legend()\n",
    "plt.xlabel('Updates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "1. A linear logistic classifier is only able to create a linear decision boundary. Change the Logistic classifier into a (nonlinear) Neural network by inserting a dense hidden layer between the input and output layers of the model\n",
    " \n",
    "2. Experiment with multiple hidden layers or more / less hidden units. What happens to the decision boundary?\n",
    " \n",
    "3. Overfitting: When increasing the number of hidden layers / units, the neural network will fit the training data better by creating a highly nonlinear decision boundary. If the model is too complex it will often generalize poorly to new data (validation and test set). Can you observe this from the training and validation errors? \n",
    " \n",
    "4. We used the vanilla stochastic gradient descent algorithm for parameter updates. This usually converges slowly and more sophisticated pseudo-second-order methods usually work better. Try changing the optimizer to [adam or momentum](http://pytorch.org/docs/master/optim.html#torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're done, continue to the next part of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
