{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(sns.dark_palette(\"purple\"))\n",
    "\n",
    "try:\n",
    "    from plotting import plot_autoencoder_stats\n",
    "except Exception as ex:\n",
    "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
    "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
    "          \\n---------------------------------------------\")\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupservised Learning \n",
    "\n",
    "## Labelling Data is Expensive\n",
    "\n",
    "In supervised machine learning, one aims at learning a mapping $f_{\\psi} : \\mathbf{x} \\in \\mathcal{R}^P \\rightarrow \\mathbf{y}$ from observations $\\mathbf{x}$ to the target $\\mathbf{y}$ using a dataset $\\mathcal{D} = \\{\\mathbf{x}_i, \\mathbf{y}_i\\}_{i=1, \\dots, N}$ of finite size N (e.g. image classification, translation). Because of the curse of dimensionality, high-dimensional inputs (images) and complex the models (deep learning) require large datasets (million of pairs $(\\mathbf{x}, \\mathbf{y})$). In practice, labelling data is expensive (e.g. marking the presence of cancer in X-ray chest scans). \n",
    "\n",
    "## Compression is Comprehension: Learning without Target\n",
    "\n",
    "In order to overcome the curse of dimensionality, we aim at learning a compressed representation $\\mathbf{z} \\in \\mathcal{R}^M$ of $\\mathbf{x}$ such that $M < P$ and there is a mapping $g_{\\phi}: \\mathbf{x} \\rightarrow \\mathbf{z}$ linking each data point to its representation. Ideally, $\\mathbf{z}$ is low-dimensional set of features which efficiently describes $\\mathbf{x}$. As an illustration, when modelling pictures of celebrities (CelebA dataset), the set of facial features (eye color, age, hair lenth, etc.) is a compressed (and lossy) representation of $\\mathbf{x}$. In practice, the representation  $\\mathbf{z}$ is unobservable and [unlikely to overlap with such known features](https://arxiv.org/abs/1811.12359). Yet, the representation $\\mathbf{z}$ is low dimensional and learning a mapping $f_{\\psi} : \\mathbf{z} \\in \\mathcal{R}^M \\rightarrow \\mathbf{y}$ is often easier.\n",
    "\n",
    "Whereas labelling the data is expensive, observations $\\mathbf{x}$ are cheap to acquire. In many cases, one can scrap the web to gather a large collection of images or text. As a result, large deep learning models can be deployed to learn $g_{\\phi}$, and smaller / data-efficient models can be applied downstream to solve the supervised task.\n",
    "\n",
    "\n",
    "\n",
    "# Auto-encoders: Compression as a Generation Process\n",
    "In this notebook you will implement a simple auto-encoder (AE). We assume that you are already familiar with the basics of neural networks. We will start by defining an AE similar to the one used for the finetuning step by [Geoffrey Hinton and Ruslan Salakhutdinov](https://www.cs.toronto.edu/~hinton/science.pdf). We will experiment with the AE setup and try to run it on the MNIST dataset. There has been a wide variety of research into the field of auto-encoders and the technique that you are about to learn is very simple compared to modern methods: Masked Autoencoders ([MADE](https://arxiv.org/abs/1502.03509), [BERT](https://arxiv.org/abs/1810.04805)) and Variational Autoencoders ([VAE](https://arxiv.org/abs/1312.6114), [VQ-VAE](https://arxiv.org/abs/1711.00937), [BIVA](https://arxiv.org/abs/1902.02102), [NVAE](https://arxiv.org/abs/2007.03898)).\n",
    "\n",
    "In unsupervised learning, we aim at learning compressed representations $\\mathbf{z} \\in \\mathcal{P}$ of $\\mathbf{x} \\in \\mathcal{R}$ where $ M < P$. The architecture of an autoencoder can be decomposed in two steps:\n",
    "\n",
    "1. *Encoding* $\\mathbf{x}$ into a low-dimensional representation $\\mathbf{z}$ using a neural network $g_{\\phi} : \\mathbf{x} \\rightarrow \\mathbf{z}$.\n",
    "2. *Decoding* the representation $\\mathbf{z}$ into a reconstruction $\\hat{\\mathbf{x}} = h_\\theta(\\mathbf{z}) \\in \\mathcal{R}^P$.\n",
    "\n",
    "Because $M < P$, the encoding acts as an information bottleneck: only part of the information describing $\\mathbf{x}$ can be encoded into $\\mathbf{z}$ as long as $M$ is sufficiently small.\n",
    "\n",
    "Learning the parameters of the autoencoder relies on two aspects:\n",
    "\n",
    "1. A distance in the observation space $d : \\mathcal{R}^{P} \\times \\mathcal{R}^{P} \\rightarrow \\mathcal{R}$ (e.g. MSE), measuring the reconstruction quality.\n",
    "2. Using backpropagation coupled with stochastic gradient descent (SGD) to optimize the parameters $\\{\\phi, \\theta\\}$ w.r.t $L := \\frac{1}{N} \\sum_i d(x_i, h_{\\theta}(g_{\\phi}(\\mathbf{x})))$.\n",
    "\n",
    "<img src=\"static/autoencoder.png\" />\n",
    "\n",
    "*The exercises are found at the bottom of the notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "First let us load the MNIST dataset and plot a few examples. In this notebook we will use the *dataloaders* and *datasets* provided by PyTorch. Defining the loading of datasets using a dataloader has the advantage that it only load the data that is *neccessary* into memory, which enables us to use very large scale datasets.\n",
    "\n",
    "We only load a limited amount of classes defined by the `classes` variable to speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Flatten the 2d-array image into a vector\n",
    "flatten = lambda x: ToTensor()(x).view(28**2)\n",
    "\n",
    "# Define the train and test sets\n",
    "dset_train = MNIST(\"./\", train=True,  transform=flatten, download=True)\n",
    "dset_test  = MNIST(\"./\", train=False, transform=flatten)\n",
    "\n",
    "# The digit classes to use\n",
    "classes = [3, 7]\n",
    "\n",
    "def stratified_sampler(labels, classes):\n",
    "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
    "    from functools import reduce\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "    indices = torch.from_numpy(indices)\n",
    "    return SubsetRandomSampler(indices)\n",
    "\n",
    "\n",
    "# The loaders perform the actual work\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
    "                          sampler=stratified_sampler(dset_train.targets, classes), pin_memory=cuda)\n",
    "test_loader  = DataLoader(dset_test, batch_size=batch_size, \n",
    "                          sampler=stratified_sampler(dset_test.targets, classes), pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a batch of MNIST examples\n",
    "f, axarr = plt.subplots(4, 16, figsize=(16, 4))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(images[i].view(28, 28), cmap=\"binary_r\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.suptitle('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "When defining the model the latent layer $z$ must act as a bottleneck of information. We initialize the AE with 1 hidden layer in the encoder and decoder using ReLU units as nonlinearities. The latent layer has a dimensionality of 2 in order to make it easy to visualise. Since $x$ are pixel intensities that are normalized between 0 and 1, we use the sigmoid nonlinearity to model the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# define size variables\n",
    "num_features = 28*28\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units, latent_features=2):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # We typically employ an \"hourglass\" structure\n",
    "        # meaning that the decoder should be an encoder\n",
    "        # in reverse.\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=num_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # bottleneck layer\n",
    "            nn.Linear(in_features=hidden_units, out_features=latent_features)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=hidden_units),\n",
    "            nn.ReLU(),\n",
    "            # output layer, projecting back to image size\n",
    "            nn.Linear(in_features=hidden_units, out_features=num_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        outputs = {}\n",
    "        # we don't apply an activation to the bottleneck layer\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # apply sigmoid to output to get pixel intensities between 0 and 1\n",
    "        x_hat = torch.sigmoid(self.decoder(z))\n",
    "        \n",
    "        return {\n",
    "            'z': z,\n",
    "            'x_hat': x_hat\n",
    "        }\n",
    "\n",
    "\n",
    "# Choose the shape of the autoencoder\n",
    "net = AutoEncoder(hidden_units=128, latent_features=2)\n",
    "\n",
    "if cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following we define the PyTorch functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# if you want L2 regularization, then add weight_decay to SGD\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.25)\n",
    "\n",
    "# We will use pixel wise mean-squared error as our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the forward pass by checking whether the output shape is the same as the as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the forward pass\n",
    "# expect output size of [32, num_features]\n",
    "x, y = next(iter(train_loader))\n",
    "print(f\"x.shape = {x.shape}\")\n",
    "\n",
    "if cuda:\n",
    "    x = x.cuda()\n",
    "\n",
    "outputs = net(x)\n",
    "print(f\"x_hat.shape = {outputs['x_hat'].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training loop we sample each batch and evaluate the error, latent space, and reconstructions on every epoch.\n",
    "\n",
    "**NOTE** this will take a while on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_loss = []\n",
    "    net.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x, y in train_loader:\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "        x_hat = outputs['x_hat']\n",
    "\n",
    "        # note, target is the original tensor, as we're working with auto-encoders\n",
    "        loss = loss_function(x_hat, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "\n",
    "    # Evaluate, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        x, y = next(iter(test_loader))\n",
    "        \n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "        \n",
    "        outputs = net(x)\n",
    "\n",
    "        # We save the latent variable and reconstruction for later use\n",
    "        # we will need them on the CPU to plot\n",
    "        x_hat = outputs['x_hat']\n",
    "        z = outputs['z'].cpu().numpy()\n",
    "\n",
    "        loss = loss_function(x_hat, x)\n",
    "\n",
    "        valid_loss.append(loss.item())\n",
    "    \n",
    "    if epoch == 0:\n",
    "        continue\n",
    "\n",
    "    # live plotting of the trainig curves and representation\n",
    "    plot_autoencoder_stats(x=x,\n",
    "                           x_hat=x_hat,\n",
    "                           z=z,\n",
    "                           y=y,\n",
    "                           train_loss=train_loss,\n",
    "                           valid_loss=valid_loss,\n",
    "                           epoch=epoch,\n",
    "                           classes=classes,\n",
    "                           dimensionality_reduction_op=None)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - Analyzing the AE\n",
    "1. The above implementation of an AE is very simple.\n",
    "    - Experiment with the number of layers and try different non-linearities in order to improve the reconstructions\n",
    "    - What happens with the network when we change the non-linearities in the latent layer (e.g. sigmoid)?\n",
    "    - Try to increase the number of digit classes in the training set and analyze the results\n",
    "    - Test different optimization algorithms such as ADAM and RMSProp and decide whether you should use regularizers\n",
    "       \n",
    "2. Currently we optimize w.r.t. mean squared error. \n",
    "    - Find another error function that could fit this problem better\n",
    "    - Evaluate whether the similarity function $d$ is a better choice and explain your findings\n",
    "\n",
    "3. Complexity of the bottleneck.\n",
    "    - Increase the number of units in the latent layer and train\n",
    "    - Visualize by using [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or [t-SNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Adding classification for semi-supervised learning\n",
    "\n",
    "The above training has been performed unsupervised. Now let us assume that we only have a fraction of labeled data points from each class. As we know, semi-supervised learning can be utilized by combining unsupervised and supervised learning. Now you must analyze whether a trained AE from the above exercise can aid a classifier.\n",
    "\n",
    "1. Build a simple classifier (like the ones from week1) where you:\n",
    "    - Train on the labeled dataset and evaluate the results\n",
    "2. Build a second classifier and train on the latent output $\\mathbf{z}$ of the AE.\n",
    "3. Build a third classifier and train on the reconstructions of the AE.\n",
    "4. Evaluate the classifiers against each other and implement a model that improves the classification by combining the input, latent output, and reconstruction.\n",
    "\n",
    "Below we provide some starting code for using only a subset of the labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def uniform_stratified_sampler(labels, classes, n=None):\n",
    "    \"\"\"\n",
    "    Stratified sampler that distributes labels uniformly by\n",
    "    sampling at most n data points per class\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    # Only choose digits in n_labels\n",
    "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
    "\n",
    "    # Ensure uniform distribution of labels\n",
    "    np.random.shuffle(indices)\n",
    "    indices = np.hstack([list(filter(lambda idx: labels[idx] == i, indices))[:n] for i in classes])\n",
    "\n",
    "    indices = torch.from_numpy(indices)\n",
    "    sampler = SubsetRandomSampler(indices)\n",
    "    return sampler\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Specify how many labelled examples we want per digit class\n",
    "labels_per_class = 10\n",
    "\n",
    "# Large pool of unlabelled data\n",
    "unlabelled = DataLoader(dset_train, batch_size=batch_size, \n",
    "                        sampler=stratified_sampler(dset_train.train_labels, classes=classes), pin_memory=cuda)\n",
    "\n",
    "# Smaller pool of labelled data\n",
    "labelled = DataLoader(dset_train, batch_size=batch_size,\n",
    "                      sampler=uniform_stratified_sampler(dset_train.train_labels, classes=classes, n=labels_per_class),\n",
    "                      pin_memory=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "# This is an example of how you can use both the labelled\n",
    "# and unlabelled loader in unison\n",
    "\n",
    "### Define your classifier ###\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Go through both labelled and unlabelled data\n",
    "    for (x, y), (u, _) in zip(cycle(labelled), unlabelled):\n",
    "        \n",
    "        if cuda:\n",
    "            x, y, u = x.cuda(), y.cuda(), u.cuda()\n",
    "        \n",
    "        # Send labelled data through autoencoder\n",
    "        outputs = net(x)\n",
    "\n",
    "        ### Define your loss function ###\n",
    "        loss = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
