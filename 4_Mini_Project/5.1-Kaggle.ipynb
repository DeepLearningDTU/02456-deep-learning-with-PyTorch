{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Originally created for a previous version of the [02456-deep-learning](https://github.com/DeepLearningDTU/02456-deep-learning) course material, but [converted to PyTorch](https://github.com/pytorch/tutorials).\n",
    "See repos for credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install pandas if you don't already have it (uncommet line below)\n",
    "\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "import data_utils\n",
    "\n",
    "#import sys\n",
    "#sys.path.append(os.path.join('.', '..')) # Allow us to import shared custom \n",
    "#                                         # libraries, like utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying everything together\n",
    "\n",
    "Now that you have learned about the three most used network architectures: FFNs, CNNs and RNNs. It is time to combine these network types into a more advanced model. \n",
    "It often happens that you have a combination of data that cannot fully be modeled by any one of these three types of network. Knowing how to divide the data into the right subsets, and then build a network that handles each subset efficiently can mean the difference between a great model and an unusable one. \n",
    "\n",
    "In this notebook we will work on the **Kaggle Leaf Classification Challenge**, a data science competition from [`kaggle.com`](kaggle.com) that contains several different kinds of data.\n",
    "First we will download the data and visualize it, and then we will train a network to classify the data.\n",
    "A simple network with poor performance in provided for you as a starting point, but it is up to you use the things you have learnt to improve the results.\n",
    "\n",
    "\n",
    "## Kaggle challenge\n",
    "Kaggle is a website to participate in real life challenges.\n",
    "Early 2017 it was bought by Google, who wanted access to the global community of data scientists it has created over the last 7 years.\n",
    "Since then Google have sponsored its expansion and now the prizes of the competitions and the amount of public datasets are bigger than ever. \n",
    "Most competitions on Kaggle have a dataset, an accuracy metric and a leaderboard to compare submissions.\n",
    "You can read more about Kagglepublic datasets [here](https://www.kaggle.com/datasets).\n",
    "\n",
    "The challenge we will pursue is the [_Leaf Classification_](https://www.kaggle.com/c/leaf-classification) challenge.\n",
    "The dataset consists approximately 1,584 images of leaf specimens (16 samples each of 99 species) which have been converted to binary black leaves against white backgrounds. \n",
    "Three sets of features are also provided per image: a shape contiguous descriptor, an interior texture histogram, and a ï¬ne-scale margin histogram. For each feature, a 64-attribute vector is given per leaf sample.\n",
    "\n",
    "We will primarily look into the type of neural network best suited for handling this type of data. \n",
    " * For images, usually the convolutional neural network does a pretty good job, \n",
    " * For timeseries (like the shape) usually the RNN is the network of choice\n",
    " * For the describing features a FFN is often a great option\n",
    "\n",
    "Lastly, we will train the model and put the outputs in a submission file that we can submit to kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get set up\n",
    "\n",
    "**NB**: You will need a Kaggle account for this exercise!\n",
    "\n",
    "1. Go to [Kaggle](https://www.kaggle.com/), create a user\n",
    "1. [Download the dataset](https://www.kaggle.com/c/leaf-classification/data)\n",
    "1. Unpack the dataset in the current directory. Structure should be as follows:\n",
    "```\n",
    "02456-deep-learning-with-PyTorch\\4_Mini_Project\n",
    "--> sample_submission.csv\n",
    "--> test.csv\n",
    "--> train.csv\n",
    "--> images\n",
    "--> --> 1.jpg\n",
    "--> --> 2.jpg\n",
    "--> --> 3.jpg\n",
    "--> --> ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data\n",
    "\n",
    "First we start out by looking at the images. \n",
    "You need to load them first!\n",
    "Then we load in the training data, which is in CSV format. For this, we use [pandas](https://pandas.pydata.org/).\n",
    "Pandas is useful for data analysis, but we don't suggest using it in any production code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(\"images/*.jpg\")\n",
    "print(\"Total Observations:\\t\", len(image_paths))\n",
    "\n",
    "# now loading the train.csv to find features for each training point\n",
    "train = pd.read_csv('train.csv')\n",
    "train_images = ['images/{}.jpg'.format(i) for i in train.id.values]\n",
    "\n",
    "# notice how we \"only\" have 990 images for training, the rest is for testing\n",
    "print(\"Training data shape:\\t\", train.shape)\n",
    "\n",
    "# now do similar as in train example above for test.csv\n",
    "test = pd.read_csv('test.csv')\n",
    "# notice that we do not have species here, we need to predict that ..!\n",
    "print(\"Test data shape:\\t\", test.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our training data and images loaded into memory.\n",
    "It is time to take a look at the data.\n",
    "Trying to classify leaves does not sound like a particularly difficult or interesting problem.\n",
    "We have probably all had teachers forcing us to do it on field trips as children.\n",
    "\n",
    "But try to take a look at the 99 different categories and come up with a system that discern all 99 types of leaves from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First we find an example of each species in order to visualize it\n",
    "species = np.array(sorted(train.species.unique()))\n",
    "species_examples = [np.random.choice(train[train.species == s].id.values) for s in species]\n",
    "\n",
    "# Then we gather its' index in our list of images in order to find the correct image\n",
    "indexes = [image_paths.index('images/{}.jpg'.format(i)) for i in species_examples]\n",
    "\n",
    "# now plot 1 image from each category\n",
    "fig = plt.figure(figsize=(50, 50))\n",
    "for i, idx in enumerate(indexes):\n",
    "    plt.subplot(10, 10, i + 1)\n",
    "    image = imread(image_paths[idx], as_gray=True)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"%s\" % (species[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, classifying leaves is actually a very tough problem.\n",
    "What makes it even worse, is that we cannot use all the image data we have available.\n",
    "In order to decrease the amount of computations needed, we need to reduce the size of the images as much as possible.\n",
    "On top of that our neural network only accepts fixed size input tensors.\n",
    "This means we will have to change the shape of the images so that they all have the same sizes.\n",
    "\n",
    "Resizing is problematic because it alters the shape of the leaves, and for some of them, this is their most distinctive feature. Take a look at `Salix_Intergra` in the bottom left corner.\n",
    "Describing this leaf without taking its' shape into account seems extremely difficult.\n",
    "\n",
    "Therefore we will first pad all the images into squares, and then resize them, as visualized below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "1.1) **Find an appropriate image size**. Test various resizings of the image until you have found the smallest resizing of the image where you \"can still differentiate between the images\".\n",
    "How small is to small should ultimately be determined by an actual test, but what makes visual sense is a good place to start.\n",
    "Change the `image_size=(?, ?)` parameter below, and note your choice.\n",
    " * **Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Image pre-processing\n",
    "image_size = (300, 300)  # <-- YOUR CODE HERE\n",
    "image_size = (32, 32)  # <-- YOUR CODE HERE\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 6))\n",
    "amount = 5\n",
    "image_sample = np.random.choice(train_images, amount)\n",
    "ax = plt.subplot(2, amount + 1, 1)\n",
    "txt = ax.text(0.4, 0.5, 'Original', fontsize=20)\n",
    "txt.set_clip_on(False)\n",
    "plt.axis('off')\n",
    "for i, path in enumerate(image_sample):\n",
    "    plt.subplot(2, amount + 1, i + 2)\n",
    "    image = imread(path, as_gray=True)    \n",
    "    plt.imshow(image, cmap='gray')\n",
    "    _id = int(path.split('/')[-1].split('.')[0])\n",
    "    plt.title(\"{0}\\nshape: {1}\".format(train[train.id == _id].species.values[0], image.shape))\n",
    "    plt.axis('off')\n",
    "    \n",
    "ax = plt.subplot(2, amount + 1, len(image_sample) + 2)\n",
    "txt = ax.text(0.4, 0.5, 'Resized', fontsize=20)\n",
    "txt.set_clip_on(False)\n",
    "plt.axis('off')\n",
    "for i, path in enumerate(image_sample):\n",
    "    i += len(image_sample) + 3\n",
    "    plt.subplot(2, amount + 1, i)\n",
    "    image = imread(path, as_gray=True)\n",
    "    image = data_utils.pad2square(image)  # Make the image square\n",
    "    image = resize(image, output_shape=image_size, mode='reflect', anti_aliasing=True)  # resizes the image\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the other features\n",
    "\n",
    "Now that we have looked at the image data we have available, it is time to take a look at the other available features. Below we choose a random subset of the training data, and visualize the 3 types of available features:\n",
    "* margin\n",
    "* shape\n",
    "* texture\n",
    "\n",
    "Try to run it a few times to try and get an understanding of how the features differ from species to species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and extract and plot columns\n",
    "X = train.values\n",
    "species = X[:, 1:2]\n",
    "margin = X[:, 2:66]\n",
    "shape = X[:, 66:130]\n",
    "texture = X[:, 130:]\n",
    "\n",
    "# let us plot some of the features\n",
    "plt.figure(figsize=(21,7)) # Set the plot size\n",
    "amount = 5 # Choose the amount of images we want to show at a time\n",
    "for i, idx in enumerate(np.random.choice(range(len(train)), amount)):\n",
    "    ax = plt.subplot(amount,4,1+i*4)\n",
    "    txt = ax.text(0.2, 0.2, species[idx][0], fontsize=20)\n",
    "    txt.set_clip_on(False)\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title('Species', fontsize=20)\n",
    "    plt.subplot(amount,4,2+i*4)\n",
    "    plt.plot(margin[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Margin', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(amount,4,3+i*4)\n",
    "    plt.plot(shape[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Shape', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(amount,4,4+i*4)\n",
    "    plt.plot(texture[idx])\n",
    "    if i == 0:\n",
    "        plt.title('Texture', fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "So far we have learned about the feed forward neural network, the convolutional neural network and the recurrent neural network.\n",
    "Given `margin` and `texture` are histograms, `shape` is a seqeunce of values\n",
    "\n",
    "2.1) How could Margin, Shape and Texture be represented for classification?\n",
    " * **Answer:**\n",
    "\n",
    "2.2) Describe what network type you would build and how you would represent the data points (image, margin, shape and texture).\n",
    " * **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Managing the data\n",
    "\n",
    "The details of the code in this section isn't that important.\n",
    "It simply manages the data in a nice way - so it is a good place to come back and look for inspiration when you going to work on your own projects\n",
    "\n",
    "## Defining the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# loading data and setting up constants\n",
    "IMAGE_SHAPE = image_size + (1, )\n",
    "\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH = \"test.csv\"\n",
    "IMAGE_PATHS = glob.glob(\"images/*.jpg\")\n",
    "NUM_CLASSES = 99\n",
    "NUM_FEATURES = 64 # for all three features, margin, shape and texture\n",
    "# train holds both X (input) and t (target/truth)\n",
    "data = data_utils.load_data(train_path=TRAIN_PATH, \n",
    "                            test_path=TEST_PATH,\n",
    "                            image_paths=IMAGE_PATHS,\n",
    "                            image_shape=IMAGE_SHAPE[:2])\n",
    "# to visualize the size of the dimensions of the data\n",
    "# print\n",
    "print(\"@@@Shape checking of data sets@@@\")\n",
    "# print\n",
    "print(\"TRAIN\")\n",
    "print(\"\\timages\\t%s%f\" % (data.train['images'].shape, data.train['images'].mean()))\n",
    "print(\"\\tmargins\\t%s\\t%f\" % (data.train['margins'].shape, data.train['margins'].mean()))\n",
    "print(\"\\tshapes\\t%s\\t%f\" % (data.train['shapes'].shape, data.train['shapes'].mean()))\n",
    "print(\"\\ttextures%s\\t%f\" % (data.train['textures'].shape, data.train['textures'].mean()))\n",
    "print(\"\\tts\\t %s\" % (data.train['ts'].shape))\n",
    "print(\"\\twhile training, batch_generator will onehot encode ts to (batch_size, num_classes)\")\n",
    "# print()\n",
    "print(\"TEST\")\n",
    "print(\"\\timages\\t%s\\t%f\" % (data.test['images'].shape, data.test['images'].mean())) \n",
    "print(\"\\tmargins\\t%s\\t%f\" % (data.test['margins'].shape, data.test['margins'].mean()))\n",
    "print(\"\\tshapes\\t%s\\t%f\" % (data.test['shapes'].shape, data.test['shapes'].mean()))\n",
    "print(\"\\ttextures%s\\t%f\" % (data.test['textures'].shape, data.test['textures'].mean()))\n",
    "print(\"\\tids\\t%s\" % (data.test['ids'].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator\n",
    "\n",
    "While training, we will not directly access the entire dataset, instead we have a `batch_generator` function to give us inputs aligned with their targets/ids in a size that our model can handle in memory (batch\\_size).\n",
    "\n",
    "Furthermore, the `batch_generator` also handles validation splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dummy_batch_gen = data_utils.batch_generator(data, batch_size=batch_size, num_classes=99, num_iterations=5e3, seed=42)\n",
    "train_batch = next(dummy_batch_gen.gen_train())\n",
    "valid_batch, i = next(dummy_batch_gen.gen_valid())\n",
    "test_batch, i = next(dummy_batch_gen.gen_test())\n",
    "\n",
    "print(\"TRAIN\")\n",
    "print(\"\\timages,\", train_batch['images'].shape)\n",
    "print(\"\\tmargins,\", train_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", train_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", train_batch['textures'].shape)\n",
    "print(\"\\tts,\", train_batch['ts'].shape)\n",
    "print()\n",
    "print(\"VALID\")\n",
    "print(\"\\timages,\", valid_batch['images'].shape)\n",
    "print(\"\\tmargins,\", valid_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", valid_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", valid_batch['textures'].shape)\n",
    "print(\"\\tts,\", valid_batch['ts'].shape)\n",
    "print()\n",
    "print(\"TEST\")\n",
    "print(\"\\timages,\", test_batch['images'].shape)\n",
    "print(\"\\tmargins,\", test_batch['margins'].shape)\n",
    "print(\"\\tshapes,\", test_batch['shapes'].shape)\n",
    "print(\"\\ttextures,\", test_batch['textures'].shape)\n",
    "print(\"\\tids,\", len(test_batch['ids']))\n",
    "# notice that mean is very different, which is why we use batch_norm in all input data in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = IMAGE_SHAPE\n",
    "features_cat_size = 3956  # <-- YOUR CODE HERE (This number will need to change as you change the network)\n",
    "\n",
    "conv_out_channels = 16\n",
    "conv_stride = 1\n",
    "kernel_size = 5\n",
    "pool_stride = 2\n",
    "\n",
    "features_rnn_size = NUM_FEATURES+NUM_FEATURES+100  # size of output of RNN\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "#         # conv layer with (5, 5) kernel, stride of 1, and padding 2 (gives same dim output)\n",
    "        self.conv_1 = Conv2d(in_channels=channels,\n",
    "                             out_channels=conv_out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=conv_stride,\n",
    "                             padding=2)\n",
    "        self.pool_1 = MaxPool2d(kernel_size=3,\n",
    "                                stride=pool_stride,\n",
    "                                padding=0)\n",
    "        \n",
    "        self.rnn_1 = GRU(input_size=1,\n",
    "                         hidden_size=100,\n",
    "                         num_layers=1,\n",
    "                         batch_first=True)\n",
    "\n",
    "        self.l_out = Linear(in_features=features_cat_size,\n",
    "                            out_features=NUM_CLASSES,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x_img, x_margin, x_shape, x_texture):\n",
    "        features = []\n",
    "        out = {}\n",
    "        \n",
    "        ## use concatenated leaf features\n",
    "        # YOUR CODE HERE vvv\n",
    "        x = torch.cat((x_margin, x_texture), dim=1)  # if you want to use features as feature vectors\n",
    "        features_vector = x\n",
    "        # YOUR CODE HERE ^^^\n",
    "        features.append(features_vector)\n",
    "        \n",
    "        \n",
    "        ## image features\n",
    "        # YOUR CODE HERE vvv\n",
    "        x = x_img.permute(0, 3, 1, 2)  # permute the dimensions, such that we have NCHW (num, channels, heigh, width)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = relu(x)\n",
    "        \n",
    "        features_img = x.view(batch_size, -1)\n",
    "        features_img = features_img\n",
    "        # YOUR CODE HERE ^^^\n",
    "        features.append(features_img)\n",
    "        \n",
    "        \n",
    "        ## use concatenated leaf features, where shape has been through an rnn\n",
    "        # YOUR CODE HERE vvv\n",
    "        x_shape.unsqueeze_(-1)\n",
    "        _, hs = self.rnn_1(x_shape)  # note, hs gives (num_layers * num_directions, batch, hidden_size)\n",
    "        hs = hs.permute(1, 0, 2)  # so, we need to move batch dim to first element\n",
    "        hs = hs.view(batch_size, -1)\n",
    "        x = torch.cat((x_margin, hs, x_texture), dim=1)\n",
    "        features_rnn = x\n",
    "        # YOUR CODE HERE ^^^\n",
    "        features.append(features_rnn)\n",
    "        \n",
    "        \n",
    "        features_final = torch.cat(features, dim=1)\n",
    "        pre_softmax = self.l_out(features_final)\n",
    "        out['out'] = softmax(pre_softmax, dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# weight_decay is equal to L2 regularization\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test network\n",
    "\n",
    "**NB** If you kernel keeps dying on the line below it is most likely because you run out of memory.\n",
    "The two most likely solutions are \n",
    "1. reduce the image size further\n",
    "1. change your network architecture such that it uses less resorces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_img_shape = tuple([batch_size] + list(IMAGE_SHAPE))\n",
    "_feature_shape = (batch_size, NUM_FEATURES)\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "# dummy data\n",
    "_x_image = get_variable(Variable(torch.from_numpy(randnorm(_img_shape))))\n",
    "_x_margin = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_shape = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "_x_texture = get_variable(Variable(torch.from_numpy(randnorm(_feature_shape))))\n",
    "\n",
    "# test the forward pass\n",
    "output = net(x_img=_x_image, x_margin=_x_margin, x_shape=_x_shape, x_texture=_x_texture)\n",
    "output['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 0.1 # 0.1 is ~ 100 samples for valition\n",
    "max_iter = 1000\n",
    "log_every = 100\n",
    "eval_every = 100\n",
    "\n",
    "def get_labels(batch):\n",
    "    return get_variable(Variable(torch.from_numpy(batch['ts']).long()))\n",
    "\n",
    "def get_input(batch):\n",
    "    return {\n",
    "        'x_img': get_variable(Variable(torch.from_numpy(batch['images']))),\n",
    "        'x_margin': get_variable(Variable(torch.from_numpy(batch['margins']))),\n",
    "        'x_shape': get_variable(Variable(torch.from_numpy(batch['shapes']))),\n",
    "        'x_texture': get_variable(Variable(torch.from_numpy(batch['textures'])))\n",
    "    }\n",
    "\n",
    "train_iter = []\n",
    "train_loss, train_accs = [], []\n",
    "valid_iter = []\n",
    "valid_loss, valid_accs = [], []\n",
    "batch_gen = data_utils.batch_generator(data,\n",
    "                                       batch_size=batch_size,\n",
    "                                       num_classes=NUM_CLASSES,\n",
    "                                       num_iterations=max_iter,\n",
    "                                       seed=42,\n",
    "                                       val_size=VALIDATION_SIZE)\n",
    "\n",
    "net.train()\n",
    "for i, batch_train in enumerate(batch_gen.gen_train()):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        for batch_valid, num in batch_gen.gen_valid():\n",
    "            output = net(**get_input(batch_valid))\n",
    "            labels_argmax = torch.max(get_labels(batch_valid), 1)[1]\n",
    "            val_losses += criterion(output['out'], labels_argmax) * num\n",
    "            val_accs += accuracy(output['out'], labels_argmax) * num\n",
    "            val_lengths += num\n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        valid_loss.append(get_numpy(val_losses))\n",
    "        valid_accs.append(get_numpy(val_accs))\n",
    "        valid_iter.append(i)\n",
    "#         print(\"Valid, it: {} loss: {:.2f} accs: {:.2f}\\n\".format(i, valid_loss[-1], valid_accs[-1]))\n",
    "        net.train()\n",
    "    \n",
    "    output = net(**get_input(batch_train))\n",
    "    labels_argmax = torch.max(get_labels(batch_train), 1)[1]\n",
    "    batch_loss = criterion(output['out'], labels_argmax)\n",
    "    \n",
    "    train_iter.append(i)\n",
    "    train_loss.append(float(get_numpy(batch_loss)))\n",
    "    train_accs.append(float(get_numpy(accuracy(output['out'], labels_argmax))))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(train_iter, train_loss, label='train_loss')\n",
    "        plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(train_iter, train_accs, label='train_accs')\n",
    "        plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "#         print(\"Train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, train_loss[-1], train_accs[-1]))\n",
    "        \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Now it is your turn!\n",
    "\n",
    "**Tip** This is very small dataset (number of observations) compared to the number of features.\n",
    "This means that overfitting will likely be an issue, and sometimes fancy tricks won't do any good. \n",
    "Keep that in mind, and always start out simple.\n",
    "\n",
    "**3.1) Improve the network**, and get as high a validation score as you can. \n",
    "When trying to improve the network nothing is sacred, you can change learning rate, try testing various learning rates, batch sizes, validation sizes, etc. \n",
    "And most importantly, the validation set is very small (only 1 sample per class), etc.\n",
    "\n",
    "To get you of to a good start we have created a list of **thing you might want to try**:\n",
    "* Add more layers (mostly fully connected and convolutional)\n",
    "* Increase or decrease the batch size \n",
    "* Use dropout (alot - e.g. between the convolutional layers)\n",
    "* Use batchnormalization (alot)\n",
    "* Try with L1 or L2 regularization (weigth decay)\n",
    "* Use only the image for training (with CNN) - comment on the increased time between iterations.\n",
    "* Change the image size to be bigger or smaller\n",
    "* Try other combinations of FFN, CNN, RNN parts in various ways (bigger is not always better)\n",
    "\n",
    "If your network is not performing as well as you would like it to, [here](http://theorangeduck.com/page/neural-network-not-working) is a great explanation of what might have gone wrong.\n",
    "\n",
    "\n",
    "**3.2) Improve Kaggle score**. Once happy try to get the best score on Kaggle for this dataset as you can (**upload** instructions below)\n",
    "You can upload your solution multiple times as you progress.\n",
    "A very good implementation would get a score between $0.04$ to $0.06$ (the smaller the better), try and see if you can get there, and explain what might have gone wrong if you cant. \n",
    "\n",
    "\n",
    "**3.3) Reflext on the process**, and how you got to your final design and discuss your final results. \n",
    "What worked, and what didn't?\n",
    "Include at least the following: \n",
    "* Description of final architecture\n",
    "* Description of training parameters\n",
    "* Description of final results (Kaggle and validation)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to Kaggle\n",
    "\n",
    "First we have to make testset predictions, then we have to place it in the submission file and the upload to kaggle for our score! You can upload at max 5 submissions a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET PREDICTIONS\n",
    "# containers to collect ids and predictions\n",
    "ids_test, preds_test = [], []\n",
    "net.eval()\n",
    "# run like with validation\n",
    "for batch_test, num in batch_gen.gen_test():\n",
    "    output = net(**get_input(batch_test))\n",
    "    y_out = output['out'].data\n",
    "\n",
    "    ids_test += batch_test['ids']\n",
    "    if num!=len(y_out):\n",
    "        # in case of the last batch, num will be less than batch_size\n",
    "        y_out = y_out[:num]\n",
    "    preds_test.append(y_out)\n",
    "preds_test = np.concatenate(preds_test, axis=0)\n",
    "assert len(ids_test) == len(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds_test, columns=data.le.classes_)\n",
    "ids_test_df = pd.DataFrame(ids_test, columns=[\"id\"])\n",
    "submission = pd.concat([ids_test_df, preds_df], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# below prints the submission, can be removed and replaced with code block below\n",
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload submission\n",
    "\n",
    "1. Go to [`https://www.kaggle.com/c/leaf-classification/submit`](https://www.kaggle.com/c/leaf-classification/submit)\n",
    "3. Click or drop your submission here (writing a description is good practice)\n",
    "4. Submit and look at where you are on the leaderboard.\n",
    "\n",
    "Success! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
