{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "2.1-FNN-NumPy.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAva8TnYFtFu",
        "colab_type": "text"
      },
      "source": [
        "# Contents and why we need this lab\n",
        "\n",
        "This lab is about implementing neural networks yourself in NumPy before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCa7HzwpFtFy",
        "colab_type": "text"
      },
      "source": [
        "# External sources of information\n",
        "\n",
        "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
        "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SjiIp-TFtF0",
        "colab_type": "text"
      },
      "source": [
        "# This notebook will follow the next steps:\n",
        "\n",
        "1. Data generation\n",
        "2. Initialization of parameters\n",
        "3. Definition of activation functions   \n",
        "4. A short explanation of numpy's einsum function\n",
        "5. Forward pass\n",
        "6. Backward pass (backward pass and finite differences)\n",
        "7. Training loop \n",
        "8. Testing your model\n",
        "9. Further extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvtTIP_qFtF2",
        "colab_type": "text"
      },
      "source": [
        "# Create an artificial dataset to play with\n",
        "\n",
        "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB-yRe4aFtF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smI4QFhzFtGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
        "    # Create covariates and response variable\n",
        "    if D1:\n",
        "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
        "        np.random.shuffle(X)\n",
        "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
        "    else:\n",
        "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
        "        np.random.shuffle(X)    \n",
        "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
        "\n",
        "    # Stack them together vertically to split data set\n",
        "    data_set = np.vstack((X.T,y)).T\n",
        "    \n",
        "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
        "    \n",
        "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
        "    train_mu = np.mean(train, axis=0)\n",
        "    train_sigma = np.std(train, axis=0)\n",
        "    \n",
        "    train = (train-train_mu)/train_sigma\n",
        "    validation = (validation-train_mu)/train_sigma\n",
        "    test = (test-train_mu)/train_sigma\n",
        "    \n",
        "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
        "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
        "\n",
        "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKvkAODOFtGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D1 = True\n",
        "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im2-MOcwFtGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train);\n",
        "    plt.scatter(x_validation[:,0], y_validation);\n",
        "    plt.scatter(x_test[:,0], y_test);\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train);\n",
        "    plt.scatter(x_validation[:,1], y_validation);\n",
        "    plt.scatter(x_test[:,1], y_test);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUMD2aNKFtGd",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbjrqcpVFtGe",
        "colab_type": "text"
      },
      "source": [
        "The steps to create a feed forward neural network are the following:\n",
        "\n",
        "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. We are going to save these numbers in a list \"L\" that is going to start with our input dimensionality (the number of features in X) and is going to finish with our output dimensionality (the size of Y). Anything in between these values are going to be hidden layers and the number of hidden units in each hidden layer is defined by the researcher. Remember that for each unit in each layer (besides the first one, according to our list L) there is a bias term.\n",
        "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
        "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
        "\n",
        "Our initialization will work as follows: \n",
        "\n",
        "For each layer of the neural network defined in L, initialize a matrix of weights of size (units_in, units_out) from a random normal distribution [np.random.normal()](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) and save them in a list called \"layers\". For each layer in our neural network, initialize a matrix of weights of size (1, units_out) as above and save them in a list called \"bias\". The function should return a tuple (layers, bias). The length of our lists must be len(L)-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrcirTTHFtGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize neural network:\n",
        "# the NN is a tuple with a list with weights and list with biases\n",
        "def init_NN(L):\n",
        "    \"\"\"\n",
        "    Function that initializes our feed-forward neural network. \n",
        "    Input: \n",
        "    L: list of integers. The first element must be equal to the number of features of x and the last element \n",
        "        must be the number of outputs in the network.\n",
        "    Output:\n",
        "    A tuple of:\n",
        "    weights: a list with randomly initialized weights of shape (in units, out units) each. The units are the ones we defined in L.\n",
        "        For example, if L = [2, 3, 4] layers must be a list with a first element of shape (2, 3) and a second elemtn of shape (3, 4). \n",
        "        The length of layers must be len(L)-1\n",
        "    biases: a list with randomly initialized biases of shape (1, out_units) each. For the example above, bias would be a list of length\n",
        "        2 with a first element of shape (1, 3) and a second element of shape (1, 4).\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        weights.append(np.random.normal(loc=0.0, scale=1.0, size=[L[i],L[i+1]])) \n",
        "        biases.append(np.random.normal(loc=0.0, scale=1.0, size=[1, L[i+1]]))     \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "# Initialize the unit test neural network:\n",
        "# Same steps as above but we will not initialize the weights randomly.\n",
        "def init_NN_UT(L):\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        weights.append(np.ones((L[i],L[i+1]))) \n",
        "        biases.append(np.ones((1, L[i+1])))     \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "# Initializer the unit test neural network\n",
        "L_UT  = [3, 5, 1]\n",
        "NN_UT = init_NN_UT(L_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrGJytZFtGm",
        "colab_type": "text"
      },
      "source": [
        "## Exercise a) Print all network parameters\n",
        "\n",
        "Make a function that prints all parameters (weights and biases) with information about in which layer the parameters are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iac-VwYGFtGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-jdEl-7FtGs",
        "colab_type": "text"
      },
      "source": [
        "# Advanced initialization schemes\n",
        "\n",
        "If we are not careful with initialization we can run into trouble with in both the forward and backward passes. We have random weights with random +/- sign so the signal we pass forward will also be random and zero on average. However, the absolute size of the signal may grow or shrink from layer to layer depending upon the absolute scale of random weights. A statistical analysis of this effect and the same effect for the backward pass are presented in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
        "\n",
        "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept constant when propagating layer to layer. The exact expressions depend upon the activation function used.\n",
        "\n",
        "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
        "\n",
        "In the linked paper, Glorot and Bengio propose that for tanh activation functions the following two alternative initializations:\n",
        "\n",
        "$$ w_{ij} \\sim U \\bigg[ -\\sqrt{\\frac{6}{(n_{in} + n_{out})}}, \\, \\sqrt{\\frac{6}{(n_{in} + n_{out})}} \\bigg] $$\n",
        "\n",
        "$$ w_{ij} \\sim N \\bigg( 0, \\, \\frac{2}{(n_{in} + n_{out})} \\bigg) \\ . $$\n",
        "\n",
        "Here $U[a,b]$ is a uniform distribution in the interval $a$ to $b$ and $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
        "\n",
        "He et.al. proposes for Rectified Linear Unit activations (ReLU) the following initialization:\n",
        "\n",
        "$$ w_{ij} \\sim U \\bigg[ -\\sqrt{\\frac{6}{n_{in}}}, \\, \\sqrt{\\frac{6}{n_{in}}} \\bigg] $$\n",
        "\n",
        "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2}{n_{in}} \\bigg) \\ . $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeyab9qFtGs",
        "colab_type": "text"
      },
      "source": [
        "## Exercise b) Glorot and He initialization\n",
        "\n",
        "Implement these initialization schemes by modifying the code given below.\n",
        "\n",
        "**NOTE:** The Gaussian is defined as $N( \\mu, \\, \\sigma^{2})$ but Numpy takes $\\sigma$ as argument.\n",
        "\n",
        "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyk01CgaFtGt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Glorot\n",
        "def init_NN_glorot_Tanh(L, uniform=False):\n",
        "    \"\"\"\n",
        "    Initializer using the glorot initialization scheme\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        if uniform:\n",
        "            bound = 1.0 # <- replace with proper initialization\n",
        "            weights.append(np.random.uniform(low=-bound, high=bound, size=[L[i],L[i+1]])) \n",
        "            biases.append(np.random.uniform(low=-bound, high=bound, size=[1, L[i+1]]))  \n",
        "        else:\n",
        "            std = 1.0 # <- replace with proper initialization\n",
        "            weights.append(np.random.normal(loc=0.0, scale=std, size=[L[i],L[i+1]])) \n",
        "            biases.append(np.random.normal(loc=0.0, scale=std, size=[1, L[i+1]]))       \n",
        "        \n",
        "    return (weights, biases)\n",
        "\n",
        "## He\n",
        "def init_NN_he_ReLU(L, uniform=False):\n",
        "    \"\"\"\n",
        "    Initializer using the He initialization scheme\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    biases  = []\n",
        "    for i in range(len(L)-1):\n",
        "        if uniform:\n",
        "            bound = 1.0 # <- replace with proper initialization\n",
        "            weights.append(np.random.uniform(low=-bound, high=bound, size=[L[i],L[i+1]])) \n",
        "            biases.append(np.random.uniform(low=-bound, high=bound, size=[1, L[i+1]]))  \n",
        "        else:\n",
        "            std = 1.0 # <- replace with proper initialization\n",
        "            weights.append(np.random.normal(loc=0.0, scale=std, size=[L[i],L[i+1]])) \n",
        "            biases.append(np.random.normal(loc=0.0, scale=std, size=[1, L[i+1]]))       \n",
        "        \n",
        "    return (weights, biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4xk_ORFtGz",
        "colab_type": "text"
      },
      "source": [
        "# Activation functions\n",
        "\n",
        "To have a full definition of the neural network, we must define an activation function for every layer in our list L (again, exluding the first term, which is the number of input dimensions). Several activation functions have been proposed and have different characteristics. Here, we will implement the linear activation function (the identity function), the sigmoid activation function (squeeshes the outcome of each neuron into the $[0, 1]$ range), the Hyperbolic Tangent (Tanh) that squeeshes the outcome of each neuron to $[-1, 1]$ and the Rectified Linear Unit (ReLU). \n",
        "\n",
        "We will also include the derivative in the function. We need this in order to do our back-propagation algorithm. Don't rush, we will get there soon. For any neural network, save the activation functions in a list. This list must be of size len(L)-1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MkoJmzaFtGz",
        "colab_type": "text"
      },
      "source": [
        "## Linear activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3NVVlnwHFtG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Linear(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Linear activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return np.ones_like(x)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mm1k6xfFtG4",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dBmjKesUFtG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Sigmoid(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Sigmoid activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    f = 1/(1+np.exp(-x))\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return f*(1-f)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ZULT1rFtG_",
        "colab_type": "text"
      },
      "source": [
        "## Hyperbolic Tangent activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fESIXSJIFtHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Tanh(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Sigmoid activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return 1-f**2\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return f"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEQqh4LFtHF",
        "colab_type": "text"
      },
      "source": [
        "## Rectifier linear unit (ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wtG8BHKZFtHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReLU(x, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the element-wise Rectifier Linear Unit activation function for an array x\n",
        "    inputs:\n",
        "    x: The array where the function is applied\n",
        "    derivative: if set to True will return the derivative instead of the forward pass\n",
        "    \"\"\"\n",
        "    \n",
        "    if derivative:              # Return the derivative of the function evaluated at x\n",
        "        return (x>0).astype(int)\n",
        "    else:                       # Return the forward pass of the function at x\n",
        "        return np.maximum(x, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_79HOAXrFtHK",
        "colab_type": "text"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RME0JjT_FtHK",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined our activation functions we can visualize them to see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oOL2UolJFtHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.linspace(-6, 6, 100)\n",
        "units = {\n",
        "    \"Linear\": lambda x: Linear(x),\n",
        "    \"Sigmoid\": lambda x: Sigmoid(x),\n",
        "    \"ReLU\": lambda x: ReLU(x),\n",
        "    \"tanh\": lambda x: Tanh(x)\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "[plt.plot(x, unit(x), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
        "plt.legend(loc=2, fontsize=16)\n",
        "plt.title('Our activation functions', fontsize=20)\n",
        "plt.ylim([-2, 5])\n",
        "plt.xlim([-6, 6])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnrJX1SGFtHR",
        "colab_type": "text"
      },
      "source": [
        "## Exercise c) Glorot initialization for all activation functions\n",
        "\n",
        "Implement a function by adding to the code snippet below that can take network L and list of activations function as argument and return a Glorot initialized network.  Hint: [This blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init) gives a table for the activation functions we use here.\n",
        "\n",
        "Briefly explain in words how these how these values are calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qW5UjBhzFtHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_NN_Glorot(L, activations, uniform=False):\n",
        "    \"\"\"\n",
        "    Initializer using the glorot initialization scheme\n",
        "    \"\"\"\n",
        "    # Insert code here\n",
        "\n",
        "# Initializes the unit test neural network\n",
        "L_UT  = [3, 5, 1]\n",
        "ACT_UT = [ReLU, Tanh]\n",
        "NN_Glorot = init_NN_Glorot(L_UT, ACT_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JlvZ6O5FtHV",
        "colab_type": "text"
      },
      "source": [
        "# Numpy einsum (EINstein SUMmation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq_yqPEIFtHX",
        "colab_type": "text"
      },
      "source": [
        "[Einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) gives us the possibility to compute almost any matrix operation in a single function. You can find a good description in the link above. Here are a few examples of some important uses:\n",
        "\n",
        "**Transpose:** We can write the transpose of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ij -> ji', A) \n",
        "```\n",
        "\n",
        "**Trace:** We can write the trace of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ii -> ', A) \n",
        "```\n",
        "\n",
        "**Diagonal:** We can write the diagonal of matrix $A$:\n",
        "\n",
        "```\n",
        "np.einsum('ii -> i', A) \n",
        "```\n",
        " \n",
        "**Matrix product:** We can write the multiplication of matrices $A$ and $B$ as:\n",
        "\n",
        "```\n",
        "np.einsum('ij, jk -> ik', A, B)\n",
        "```\n",
        "\n",
        "Note that $j$ in both matrices $A$ and $B$ should be the same size. \n",
        "\n",
        "**Batched matrix product (or why bothering):** All of the functions we performed above are built in numpy (np.tranpose, np.trace, np.matmul), however, when you want to do more complex operations, it might become less readable and computationaly efficient. Let's introduce a three dimensional matrix $H$ with indices $b,j,k$, where the first dimension is the batch (training example) dimension. In einsum, we can then write:\n",
        "\n",
        "```\n",
        "np.einsum('ij, bjk -> bik', A, H)\n",
        "```\n",
        "\n",
        "In order to perform a batched matrix multiplication where we multiple over the second dimension in the first marix and second dimension in the second matrix. The result is a new three dimensional matrix where the first dimension is the first dimension from $H$ and second is the first dimension from $A$ and last dimension the last dimension from $H$. This is a very simple one line (and readable) way to do matrix operations that will be very useful for neural network code. \n",
        "\n",
        "\n",
        "#### _**Tips and tricks when using einsum**_\n",
        "\n",
        "At the beginning, einsum might be a bit difficult to work with. The most important thing to do when using it is keeping track of the dimensions of your input and output matrices. An easy way to keep track of these dimensions is by using some sort of naming convention. Just like in the batched matrix product above we used $b$ to denote the batch dimension. In all the functions of this notebook, we leave some convention of names of indexes for the einsum in the explanation of the functions. We hope you find them useful!\n",
        "\n",
        "There are some other useful resources to understand numpy.einsum:\n",
        "\n",
        "* [Olexa Bilaniuk's great blogpost on einsum]( https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/ )\n",
        "* [Stackoverflow answer to: Understanding NumPy's einsum]( https://stackoverflow.com/q/26089893/8899404 )\n",
        "* [Jessica Stringham post on einsum]( https://jessicastringham.net/2018/01/01/einsum/ )\n",
        "* [Slides of einstein summation from oxford]( http://www-astro.physics.ox.ac.uk/~sr/lectures/vectors/lecture10final.pdfc )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTMdY5DzFtHY",
        "colab_type": "text"
      },
      "source": [
        "# Forward pass\n",
        "\n",
        "The forward pass has been implemented for you. Please note how we have used einsum to perform the affine tranformation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTfP_fZEFtHY",
        "colab_type": "text"
      },
      "source": [
        "#### Indices convention.\n",
        "\n",
        "* $i$: input - layer $l$ dimension.\n",
        "* $o$: output - layer $l+1$ dimension.\n",
        "* $b$: batch size dimension.\n",
        "\n",
        "<u>Attention</u>! \n",
        "\n",
        "By convention we consider column vectors.\n",
        "Depending on your implementation,\n",
        "sometimes you will need to transpose the matrix/vector dimensions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnlRKk-RFtHa",
        "colab_type": "text"
      },
      "source": [
        "#### Matrices, Sums and Indices\n",
        "\n",
        "<u>Remember</u>!\n",
        "\n",
        "When we compute a matrix-vector product, the inner indices need to match; if we have $W \\in R^{K \\times J}$ and $z \\in R^{I}$, we can compute the matrix-vector product only if\n",
        "   *  $J=I$, then $Wz$ or \n",
        "   *  $K=I$, then $W^Tz$.\n",
        "   \n",
        "We need to transpose the matrix in the second case. Why?. Hint: inner indices matching).\n",
        "In general these two matrix-vector products are different. So pay attention to dimensions!\n",
        "\n",
        "Similarly, when we sum a matrix and a vector over a dimension, we can sum only if the dimensions match. Given the summation:\n",
        "    $$\\sum_{i=1}^I w_{ki}~z_{i},$$\n",
        "we can sum only if $J=I$.\n",
        "\n",
        "Index Contraction: after summing over an index, the index is contracted and the output is no more a function of that index. This means that if we sum over $i$, the result will be an object without index $i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAqTEP5GFtHb",
        "colab_type": "text"
      },
      "source": [
        "#### Unpacking the forward pass. \n",
        "\n",
        "For each layer we want to compute a transformation between the activated units $z$ and the layer parameters $(W, b)$. In particular such transformation is an affine one, of the form:\n",
        "$$a^{(l)} = W^{(l)} z^{(l-1)} + b^{(l)},$$\n",
        "followed by a non-linear function (activation)\n",
        "$$z^{(l)} = h(a^{(l)}).$$\n",
        "\n",
        "If $W_{oi}$ is an element in a matrix with $O$ rows and $I$ columns, \n",
        "$z_{i}$ is a value in a vector of $I$ units and \n",
        "$b_o$ is a value in a vector of $O$ biases, \n",
        "we can write the affine transformation in different ways:\n",
        "\n",
        "* *Explicit notation*: $$\\sum_{i=1}^{I} w_{oi}~z_{i} + b_o,   \\quad \\forall o=1,...O$$\n",
        "* *Matrix notation*: $$ Wz + b$$ or $$z^{T}W + b$$\n",
        "\n",
        "\n",
        "Given that we have only two indices ($i$ and $o$), after contracting $i$, the output will have dimension $O$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSelaywwFtHd",
        "colab_type": "text"
      },
      "source": [
        "#### Not so fast!\n",
        "\n",
        "In Deep Learning we have another dimension: the batch size for the data $x$. \n",
        "This additional dimension is needed to pack together samples from the dataset and parallelize computations.\n",
        "\n",
        "So we typically work with matrices $W_{io}$ (notice how the indices are switched now) and vectors $z_i$ in batches, that we write $X = \\{x_{bi}\\}^{B, I}_{i=0, b=0}$ or $Z = \\{z_{bi}\\}^{B, I}_{i=0, b=0}$.\n",
        "\n",
        "Why do we use $W_{io}$ and not $W_{oi}$ as before? In principle we can use both. In practice, given that we want the batch size as first dimension, it is simpler to use $W_{io}$ to match the inner dimensions.\n",
        "As we said before, depending on your specific implementation, you will need to transpose some matrices.\n",
        "\n",
        "$x_{bi}$ means that we process in parallel a batch of $B$ samples (for example $B$=64 images, where each image is a sample) with dimensionality $I$ (for images this dimension is between $10^{3}$ - $10^{6}$). The batch size cannot be too large (Why?) and shouldn't be too small (Why?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKgTZL-WFtHe",
        "colab_type": "text"
      },
      "source": [
        "#### einsum\n",
        "\n",
        "If you try to write in numpy a matrix-matrix product or a matrix-tensor product, you will notice that things get coumbersome fast. Additionally, your data could be a structured object with 3/4/5 dimensions ($x_{bijk}$). It's easy to get confused and make some mistakes, in particular when some of such dimensions have the same numerical value but different meaning (imagine a batch of 100 samples with dimension 100).\n",
        "\n",
        "The **einsum** function is explicitly summing over the dimension of choice taking care of the details related to batching in an efficient way. There is a striking similarity between the einsum notation and the explicit notation we have seen before. \n",
        "\n",
        "In the simple example we are working on, using einsum, matmul, or summing over an index is basically the same. \n",
        "For more complex problems in computer vision, natural language processing, generative modeling, etc. einsum helps dealing with the details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BRU4bAVFtHe",
        "colab_type": "text"
      },
      "source": [
        "#### Putting it all together.\n",
        "\n",
        "Ok! Here we are. Let's write again the explicit form for the batch case. As before, $W \\in R^{I \\times O}$ (or $W^{T} \\in R^{O \\times I}$) is a matrix, $z_{bi}$ is a matrix (or a collection of $b$ row vectors), and $b_o$ is a vector (in practice the bias vector will be broadcasted for all the samples in a batch).\n",
        "\n",
        "* *Explicit notation*: \n",
        "$$\\sum_{i=1}^I z_{bi}~w_{io} + b_{o}, \\quad \\forall o, \\quad \\forall b$$\n",
        "Before, contracting $i$, we ended up with an object of dimension $o$. Now we end up with an object of dimensions $b~\\times~o$.\n",
        "\n",
        "\n",
        "* *Matrix notation*:\n",
        "$$ Z W + b $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wQC5NSwQFtHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_pass(x, NN, activations):\n",
        "    \"\"\"\n",
        "    This function performs a forward pass. \n",
        "    It saves lists for both affine transforms of units (a) and activated units (z)\n",
        "    Input:\n",
        "    x: The input of the network             (np.array of shape: (batch_size, number_of_features))\n",
        "    NN: The initialized neural network      (tuple of list of matrices)\n",
        "    activations: the activations to be used (list of functions, same len as NN)\n",
        "\n",
        "    Output:\n",
        "    a: A list of affine transformations, that is, all x*w+b.\n",
        "    z: A list of activated units (ALL activated units including input x and output y).\n",
        "    \n",
        "    Shapes for the einsum:\n",
        "    b: batch size\n",
        "    i: size of the input hidden layer (layer l)\n",
        "    o: size of the output (layer l+1)\n",
        "    \"\"\"\n",
        "    z = [x]\n",
        "    a = []\n",
        "    \n",
        "    for l in range(len(NN[0])):\n",
        "        \n",
        "        # layer l parameters (W, bias)\n",
        "        W = NN[0][l]\n",
        "        bias = NN[1][l]\n",
        "        \n",
        "        # \\sum_{i} z^{l}_{bi} W^{l}_{io} in explicit notation\n",
        "        # z * W                          in matrix notation\n",
        "        Wz = np.einsum('bi, io -> bo', z[l], W)\n",
        "        \n",
        "        # z * W + bias\n",
        "        Wzb = Wz + bias\n",
        "        \n",
        "        a.append(Wzb)                  # The affine transform z*w+bias\n",
        "        z.append(activations[l](a[l])) # The non-linearity    \n",
        "    \n",
        "    return a, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XyXBD37FtHk",
        "colab_type": "text"
      },
      "source": [
        "# Forward pass unit test\n",
        "\n",
        "Below is a piece of code that takes a very particular setting of the network and inputs and test whether it gives the expected results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0miqRUAFtHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ACT_F_UT = [Linear, Linear]\n",
        "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT) # input has shape (1, 3) 1 batch, 3 features\n",
        "\n",
        "# Checking shapes consistency\n",
        "assert np.all(test_z[0]==np.array([1,1,1])) # Are the input vector and the first units the same?\n",
        "assert np.all(test_z[1]==test_a[0])         # Are the first affine transformations and hidden units the same?\n",
        "assert np.all(test_z[2]==test_a[1])         # Are the output units and the affine transformations the same?\n",
        "\n",
        "# Checking correctnes of values\n",
        "# First layer, calculate np.sum(np.array([1,1,1])*np.array([1,1,1]))+1 = 4\n",
        "assert np.all(test_z[1] == 4.)\n",
        "# Second layer, calculate np.sum(np.array([4,4,4,4,4])*np.array([1,1,1,1,1]))+1 = 21\n",
        "assert np.all(test_z[2] == 21.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faCxhfFnFtHp",
        "colab_type": "text"
      },
      "source": [
        "# Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JAc_ApuFtHq",
        "colab_type": "text"
      },
      "source": [
        "In order to perform a backward pass we need to define a loss function and its derivative with respect to the output of the neural network $y$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2eDYKvAFtHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def squared_error(t, y, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the squared error function and its derivative \n",
        "    Input:\n",
        "    t:      target (expected output)          (np.array)\n",
        "    y:      output from forward pass (np.array, must be the same shape as t)\n",
        "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
        "    \"\"\"\n",
        "    if np.shape(t)!=np.shape(y):\n",
        "        print(\"t and y have different shapes\")\n",
        "    if derivative: # Return the derivative of the function\n",
        "        return (y-t)\n",
        "    else:\n",
        "        return 0.5*(y-t)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrwSJ2UWFtHu",
        "colab_type": "text"
      },
      "source": [
        "## Exercise d) Implement cross entropy loss\n",
        "\n",
        "Insert code below to implement cross-entropy loss for general dimensionality of $t$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6nMuxyfzFtHv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy_loss(t, y, derivative=False):\n",
        "    \"\"\"\n",
        "    Computes the cross entropy loss function and its derivative \n",
        "    Input:\n",
        "    t:      target (expected output)          (np.array)\n",
        "    y:      output from forward pass (np.array, must be the same shape as t)\n",
        "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
        "    \"\"\"\n",
        "    ## Insert code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fAF5ew4FtHy",
        "colab_type": "text"
      },
      "source": [
        "# Backward pass "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0W3mxGFtHz",
        "colab_type": "text"
      },
      "source": [
        "## Exercise e) Complete code for backward pass\n",
        "\n",
        "Below is a implementation of the backward pass with some lines removed. Insert the missing lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oUWm98uFtH0",
        "colab_type": "text"
      },
      "source": [
        "#### Useful resources.\n",
        "\n",
        "Math intro:\n",
        "* https://www.deeplearningbook.org/contents/mlp.html\n",
        "* http://neuralnetworksanddeeplearning.com/chap2.html\n",
        "* http://pandamatak.com/people/anand/771/html/node37.html\n",
        "\n",
        "Code intro:\n",
        "* https://cs231n.github.io/neural-networks-case-study/#grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-Tp1sP4FtH1",
        "colab_type": "text"
      },
      "source": [
        "<u>Attention!</u>\n",
        "\n",
        "This is a difficult topic. It's normal to be confused and not immediately grasp the complete algorithm. Just work your way through it writing down the equations, writing down a small network explicitly, and thinking about the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWme3YQcFtH2",
        "colab_type": "text"
      },
      "source": [
        "#### Unpacking the backward pass.\n",
        "\n",
        "Before we learned how to **forward** data input $X$ in a parametric model $W$ to obtain a final output $y$ and build a loss function $E(t, y)$ that measures how far our prediction $y$ is from the target $t$.\n",
        "For a generic FFN at layer $l$ we can write:\n",
        "\n",
        "$$a^{(l)} = W^{(l)} z^{(l-1)} + b^{(l)},$$\n",
        "$$z^{(l)} = h(a^{(l)}),$$\n",
        "where $z^{(0)}=x$ and $z^{(L)} = y$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5YDMa9lFtH2",
        "colab_type": "text"
      },
      "source": [
        "#### Backpropagation.\n",
        "\n",
        "As the name suggests, the backpropagation algorithm is a procedure to adjust the model parameters $(W, b)$ \n",
        "<u>*propagating backward a measure of error*</u> such that our prediction $y$ is as close as possible to the target $t$.\n",
        "\n",
        "The proxy we use to measure closeness between $y$ and $t$ is a loss function $E(t, y)$. \n",
        "\n",
        "<u>Attention!</u>\n",
        "This loss is a function of the model parameters through $y = g(x, w, b)$. \n",
        "When we optimize wrt. the model parameters $(W, b)$ we can write the loss as a function of such parameters $E(w, b)$ because $y=g(w, b)$ and $t$ and $x$ are fixed input and output.\n",
        "\n",
        "In practice, backpropagation is a gradient based optimization strategy. \n",
        "Our goal is to compute all the partial derivatives $\\dfrac{\\partial E(w, b)}{\\partial w^l_{ji}}$ and $\\dfrac{\\partial E(w, b)}{\\partial b^l_{j}}$ for each layer $l$ and each unit $i$ and $j$ in the network.\n",
        "\n",
        "Computing the partial derivative wrt. the parameters directly is not easy. \n",
        "The loss depends in a non-linear and hierarchical way from the parameters. What backpropagation give us is an efficient and systematic way to compute the partial derivative as a function of intermediate quantities. The most important of such quantities is the error $\\delta_l$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-a_KrnvFtH3",
        "colab_type": "text"
      },
      "source": [
        "#### Error propagation - final layer $L$.\n",
        "\n",
        "Consider the final layer $L$.\n",
        "The partial derivatives can be written as:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial w^{L}_{ji}} =\n",
        "\\dfrac{\\partial E}{\\partial z^{L}_j}\n",
        "\\dfrac{\\partial z^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\left(\n",
        "\\color{blue}{\\dfrac{\\partial E}{\\partial z^{L}_j}\n",
        "\\dfrac{\\partial z^{L}_j}{\\partial a^{L}_j}}\n",
        "\\right)\n",
        "\\dfrac{\\partial a^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\color{blue}{\\delta^{L}_j}~\\dfrac{\\partial a^{L}_j}{\\partial w^{L}_{ji}} = \n",
        "\\delta^{L}_j~z^{L-1}_{i},\n",
        "$$\n",
        "\n",
        "where\n",
        "$a^{L}_j = \\sum_i w^{L}_{ji}~z^{L-1}_i + b^{L}_j$ and $z^{L}_j = h(a^{L}_j).$ \n",
        "\n",
        "$\\delta^{L}_j = \\partial E / \\partial a^{L}_j$ is the error at layer $L$ for unit $j$ and it is what we want to backpropagate. \n",
        "You see that it can be easily computed: \n",
        "$ \\partial E / \\partial z^{L}_j$\n",
        "is just the partial derivative of the loss wrt. to the activation.\n",
        "$ \\partial z^{L}_j / \\partial a^{L}_j$\n",
        "is the partial derivative of the activation wrt. the output of the affine transformation.\n",
        "You already know how to compute these derivative (How?) and consequently you know $\\delta^{L}_j$.\n",
        "\n",
        "For the biases in the final layer the computation is basically the same:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial b_{j}} =\n",
        "\\left(\\dfrac{\\partial E}{\\partial z_j}\n",
        "\\dfrac{\\partial z_j}{\\partial a_j}\\right)\n",
        "\\dfrac{\\partial a_j}{\\partial b_{j}} = \n",
        "\\delta_j~\\dfrac{\\partial a_j}{\\partial b_{j}} = \n",
        "\\delta_j.\n",
        "$$\n",
        "\n",
        "You can see that $\\delta_j$ is the same we computed, and the only thing that changes is the partial derivative wrt $w_{ji}$ and $b_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOvIa4zKFtH4",
        "colab_type": "text"
      },
      "source": [
        "#### Error propagation - layer $l$.\n",
        "\n",
        "For a generic layer $l$ we want to derive a recursive formula for the error $\\delta^{l}_j$.\n",
        "In particular, if we at layer $l$ and unit $j$, we consider the $k$ downstream units influenced by $j$:\n",
        "\n",
        "$$\\dfrac{\\partial E(w, b)}{\\partial w^l_{ji}} =\n",
        "\\sum_k\n",
        "\\dfrac{\\partial E}{\\partial a^{l+1}_k}\n",
        "\\dfrac{\\partial a^{l+1}_k}{\\partial w^l_{ji}} =\n",
        "\\left(\n",
        "\\color{green}{\\sum_k\n",
        "\\dfrac{\\partial E}{\\partial a^{l+1}_k}\n",
        "\\dfrac{\\partial a^{l+1}_k}{\\partial z^l_j}\n",
        "\\dfrac{\\partial z^l_j}{\\partial a^l_j}}\n",
        "\\right)\n",
        "\\dfrac{\\partial a^l_j}{\\partial w^l_{ji}} = \n",
        "\\color{green}{\\delta^{l}_j}~\\dfrac{\\partial a^{l}_j}{\\partial w^{l}_{ji}} = \n",
        "\\delta^l_j~z^{l-1}_{i},\n",
        "$$\n",
        "\n",
        "where $a^{l}_j = \\sum_i w^{l}_{ji}~z^{l-1}_i + b^{l}_j$ and $z^{l}_j = h(a^{l}_j).$\n",
        "\n",
        "Notice how the terms in $\\delta^{l}_j$ are easy to compute: $\\partial E  / \\partial a^{l+1}_k = \\delta^{l+1}_k$ is known (we start from the last layer). $\\partial a^{l+1}_k / \\partial z^{l}_j$ is something new, but we can easily compute this term too. \n",
        "We notice that:\n",
        "$$a^{l+1}_k = \\sum_j w^{l+1}_{kj} z^{l}_{j} + b_k,$$\n",
        "and $\\partial a^{l+1}_k / \\partial z^{l}_j = w^{l+1}_{kj}$. \n",
        "Finally $\\partial z^l_j / \\partial a^l_j$ is just the partial derivative of the activation (as before).\n",
        "\n",
        "We can now write the general recursive form for error propagation:\n",
        "\n",
        "$$\\delta^{l}_j = \\left(\\sum_k \\delta^{l+1}_k w^{l+1}_{kj}\\right) \\dfrac{\\partial z^l_j}{\\partial a^l_j}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDjCFCCDFtH5",
        "colab_type": "text"
      },
      "source": [
        "#### Putting it all together.\n",
        "\n",
        "Here we are! Everything boils down to recursively finding the errors $\\delta^{l}$ starting from the last layer $L$ backward. When you have $\\delta^{l}$, you just need a final multiplication with activations to find the partial derivative wrt the parameters for the layer.\n",
        "Let's write the steps in matrix form:\n",
        "\n",
        "* Errror for the last layer $L$:\n",
        "$$\\delta^{L} = \\nabla_z E \\circ h'(a^{L}).$$\n",
        "* Error for any layer $l$:\n",
        "$$\\delta^l = ((W^{l+1})^{T} \\delta^{l+1}) \\circ h'(a^{l}).$$\n",
        "* Partial derivative cost for $w$: \n",
        "$$\\dfrac{\\partial E}{\\partial w^{l}} = \\delta^{l} z^{l-1}.$$\n",
        "* Partial derivative cost for $b$:\n",
        "$$\\dfrac{\\partial E}{\\partial b^{l}} = \\delta^{l}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E5VYbaVqFtH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_pass(x, t, y, z, a, NN, activations, loss_f):\n",
        "    \"\"\"\n",
        "    This function performs a backward pass ITERATIVELY. It saves lists all of the derivatives in the process\n",
        "    \n",
        "    Input:\n",
        "    x:           The input used for the batch                (np.array)\n",
        "    t:           The observed targets                        (np.array, the first dimension must be the same to x)\n",
        "    y:           The output of the forward_pass of NN for x  (np.array, must have the same shape as t)\n",
        "    \n",
        "    a:           The affine transforms from the forward_pass (np.array) # a^{l+1}= W^{l} z^{l} + b^{l}\n",
        "    z:           The activated units from the forward_pass (np.array)   # z^{l+1}=f(a^{l+1})\n",
        "    \n",
        "    activations: The activations to be used                  (list of functions)\n",
        "    loss_f:        The loss function to be used                (one function)\n",
        "    \n",
        "    Output:\n",
        "    g_w: A list of gradients for every weight\n",
        "    g_b: A list of gradients for every bias\n",
        "    \n",
        "    Shapes for the einsum:\n",
        "    b: batch size\n",
        "    i: size of the input hidden layer (layer l)\n",
        "    o: size of the output (layer l+1)\n",
        "    \"\"\"\n",
        "    \n",
        "    BS = x.shape[0] # Implied batch shape \n",
        "    \n",
        "    # Process the last layer - Reference: Error propagation - final layer $L$.\n",
        "    \n",
        "    # First, let's compute the list of derivatives of z with respect to a\n",
        "    # these derivative are standardized and automatically handled by the activations functions defined above.\n",
        "    d_a = []\n",
        "    # dz/da\n",
        "    for i in range(len(activations)):\n",
        "        d_za = activations[i](a[i], derivative=True)\n",
        "        d_a.append(d_za)\n",
        "    \n",
        "    # Second, let's compute the derivative of the loss function with respect to z\n",
        "    # targets\n",
        "    t = t.reshape(BS, -1)\n",
        "    \n",
        "    # derivative loss wrt y\n",
        "    # dE/dy  where y=z[-1]\n",
        "    d_loss = 0      # <- Insert correct expression here\n",
        "    \n",
        "     \n",
        "    # Third, let's compute the derivative of the biases and the weights\n",
        "    g_w   = [] # List to save the gradient w.r.t. the weights\n",
        "    g_b   = [] # List to save the gradients w.r.t. the biases\n",
        "    \n",
        "    # delta : measure of error in the final layer L\n",
        "    # delta = dE/dy * dy/da\n",
        "    delta = np.einsum('bo, bo -> bo', d_loss, d_a[-1]) # loss shape: (b, o); pre-activation units shape: (b, o) hadamard product\n",
        "    \n",
        "    # affine transformation\n",
        "    # a = W z + b\n",
        "    \n",
        "    # dE/dw = (dE/dy * dy/da) da/dw = delta * da/dw\n",
        "    # notice how the gradients wrt the weights have dimension (batch, input_dim, output_dim)\n",
        "    g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-2]), axis=0)) # delta shape: (b, o), activations shape: (b, h)\n",
        "    \n",
        "    # dE/db = (dE/dy * dy/da) da/db = delta * da/db\n",
        "    g_b.append(np.mean(delta, axis=0))\n",
        "    \n",
        "    \n",
        "    # Process all the other layers - Reference: Error propagation - layer $l$\n",
        "    for l in range(1, len(NN[0])):\n",
        "        \n",
        "        W = NN[0][-l] \n",
        "        # dE/dz^{l} = dE/da^{l+1} * da^{l+1}/dz^{l} = delta^{l+1} * w^{l+1}\n",
        "        d_E_d_z = np.einsum('bo, io -> bi', delta, W)          # Derivative of the loss with respect to an activated layer d_E_d_z. \n",
        "                                                               #  delta shape: as above; weights shape: (i, o)\n",
        "                                                               # Delta: d_E_d_z (element-wise mult) derivative of the activation layers\n",
        "                                                               #  delta shape: as above; d_z shape: (b, i)  \n",
        "        \n",
        "        # delta : measure of error for a generic layer l \n",
        "        # dE/dz * dz/da \n",
        "        delta = 0      # <- Insert correct expression here \n",
        "        \n",
        "        # affine transformation\n",
        "        # a = Wz + b\n",
        "        \n",
        "        # dE/dw = delta * da/dw\n",
        "        g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-l-2]), axis=0)) # Derivative of cost with respect to weights in layer l:\n",
        "        \n",
        "        # dE/db = delta                                                                 # delta shape: as above; activations of l-1 shape: (b, i)\n",
        "        g_b.append(np.mean(delta, axis=0))\n",
        "        \n",
        "    return g_b[::-1], g_w[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7d7qK0uFtH9",
        "colab_type": "text"
      },
      "source": [
        "# Backward pass unit test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGiqQ0ePFtH9",
        "colab_type": "text"
      },
      "source": [
        "We are going to perform the unit test of the backward pass with a finite difference estimation, make sure to read the description of the function and that you understand it well:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiJDAZT6FtH_",
        "colab_type": "text"
      },
      "source": [
        "## Exercise f) Test correctness of derivatives with finite difference method\n",
        "\n",
        "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to test whether the backpropation implementation is working. In short we will use\n",
        "$$\n",
        "\\frac{\\partial E(w)}{\\partial w_{ij}^{(l)}} \\approx \\frac{E(v)-E(w)}{dw}\n",
        "$$\n",
        "for $dw \\ll 1$ and $v$ is the same network as $w$ apart from $v_{ij}^{(l)} = w_{ij}^{(l)} + dw$.\n",
        "\n",
        "As arguments the function should take: some data $x$ and $t$ as in the example above, the network including activations, the indices $i$, $j$, $l$ of the weight we investigate and $dw$ and return the right hand side of the expression above.\n",
        "\n",
        "_Insert your code in the cell below._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om8UfjeZFtH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert your finite difference code here\n",
        "def finite_difference(x, t, NN, activations, indexes, dw=1e-10):\n",
        "    \"\"\"\n",
        "    This function compute the finite difference between\n",
        "    \n",
        "    Input:\n",
        "    x:           The input used for the batch                (np.array)\n",
        "    t:           The observed targets                        (np.array, the first dimension must be the same to x)\n",
        "    \n",
        "    NN: The initialized neural network                       (tuple of list of matrices)\n",
        "    activations: The activations to be used                  (list of functions)\n",
        "    \n",
        "    indexes: the indexes of the parameter we want to perturb (tuple of integers)\n",
        "             v^{l}_{ji} = w^{l}_{ji} + dw\n",
        "    \n",
        "    dw: the size of the difference                           (float)\n",
        "    \n",
        "    Output:\n",
        "    finite_difference: the magnitude of the difference       (float) \n",
        "    \"\"\"\n",
        "    \n",
        "    from copy import deepcopy\n",
        "    # l layer\n",
        "    # i input dim\n",
        "    # j output dim\n",
        "    \n",
        "    (l, i, j) = indexes\n",
        "    \n",
        "    _, zv = forward_pass(x, NN, activations)\n",
        "    Ev=squared_error(t, zv[-1])\n",
        "    \n",
        "    NNw = deepcopy(NN)\n",
        "    NNw[0][l][i, j] = 0 # <- Insert correct expression \n",
        "    \n",
        "    _, zw = 0           # <- Insert correct expression\n",
        "    Ew= 0               # <- Insert correct expression\n",
        "    \n",
        "    finite_difference = (Ev - Ew) / dw\n",
        "    \n",
        "    return finite_difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcLlsejQFtID",
        "colab_type": "text"
      },
      "source": [
        "Once you have implemented the function you can compare this number with the left hand side computed by the implementation above.\n",
        "\n",
        "Try for different parameters and different values of $dw$. Scan over a range of $dw$ values. Why does the method break down for really small $dw$?\n",
        "\n",
        "_Insert your written answer here._\n",
        "\n",
        "Finite differences gives us gradients without computing gradients explicitly. Why don't we use it in practice then?\n",
        "\n",
        "_Insert your written answer here._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ59iNHmFtID",
        "colab_type": "text"
      },
      "source": [
        "Below is reference code that computes the finite differences for all parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zie8Q8xFtIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def finite_diff_grad(x, NN, ACT_F, epsilon=None):\n",
        "    \"\"\"\n",
        "    Finite differences gradient estimator: https://en.wikipedia.org/wiki/Finite_difference_method\n",
        "    The idea is that we can approximate the derivative of any function (f) with respect to any argument (w) by evaluating the function at (w+e)\n",
        "    where (e) is a small number and then computing the following opertion (f(w+e)-f(w))/e . Note that we would need N+1 evaluations of\n",
        "    the function in order to compute the whole Jacobian (first derivatives matrix) where N is the number of arguments. The \"+1\" comes from the\n",
        "    fact that we also need to evaluate the function at the current values of the argument.\n",
        "    \n",
        "    Input:\n",
        "    x:       The point at which we want to evaluate the gradient\n",
        "    NN:      The tuple that contains the neural network\n",
        "    ACT_F:   The activation functions in order to perform the forward pass\n",
        "    epsilon: The size of the difference\n",
        "    \n",
        "    Output:\n",
        "    Two lists, the first one contains the gradients with respect to the weights, the second with respect to the biases\n",
        "    \"\"\"\n",
        "    from copy import deepcopy\n",
        "    \n",
        "    if epsilon == None:\n",
        "        epsilon = np.finfo(np.float32).eps # Machine epsilon for float 32\n",
        "        \n",
        "    grads = deepcopy(NN)               # Copy of structure of the weights and biases to save the gradients                        \n",
        "    _ , test_z = forward_pass(x, NN_UT, ACT_F_UT) # We evaluate f(x)\n",
        "    \n",
        "    for e in range(len(NN)):                       # Iterator over elements of the NN:       weights or biases\n",
        "        for h in range(len(NN[e])):                # Iterator over the layer of the element: layer number\n",
        "            for r in range(NN[e][h].shape[0]):     # Iterator over                           row number\n",
        "                for c in range(NN[e][h].shape[1]): # Iterator over                           column number \n",
        "                    NN_copy             = deepcopy(NN)    \n",
        "                    NN_copy[e][h][r,c] += epsilon\n",
        "                    _, test_z_eps       = forward_pass(x, NN_copy, ACT_F)     # We evaluate f(x+eps)\n",
        "                    grads[e][h][r,c]    = (test_z_eps[-1]-test_z[-1])/epsilon # Definition of finite differences gradient\n",
        "    \n",
        "    return grads[0], grads[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GxkISRCmFtIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Unit test \n",
        "\n",
        "## First lest's compute the backward pass using our own function\n",
        "# Forward pass\n",
        "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT)\n",
        "# Backward pass\n",
        "test_g_b, test_g_w = backward_pass(np.array([[1,1,1]]), np.array([20]), test_a[-1], test_z, test_a, NN_UT, ACT_F_UT, squared_error)\n",
        "# Estimation by finite differences\n",
        "test_fdg_w, test_fdg_b = finite_diff_grad(np.array([[1,1,1]]), NN_UT, ACT_F_UT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ylz9fQHsFtIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test whether the weights and biases are all equal as the ones we estimated using back propagation\n",
        "for l in range(len(test_g_w)):\n",
        "    assert np.allclose(test_fdg_w[l], test_g_w[l])\n",
        "    assert np.allclose(test_fdg_b[l], test_g_b[l])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgBi8GOSFtIN",
        "colab_type": "text"
      },
      "source": [
        "# Training and validation\n",
        "\n",
        "We are ready to train some neural networks! Below we give some example initializations and a training loop. Try it out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWYpdw6FtIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize an arbitrary neural network\n",
        "#L  = [3, 16, 1]\n",
        "L  = [1, 8, 1]\n",
        "NN = init_NN(L)\n",
        "#NN = init_NN_glorot(L, uniform=True)\n",
        "#NN = init_NN_he_ReLU(L, uniform=True)\n",
        "\n",
        "ACT_F = [ReLU, Linear]\n",
        "#ACT_F = [Tanh, Linear]\n",
        "\n",
        "# Recommended hyper-parameters for 1-D: \n",
        "# L  = [1, 8, 1]\n",
        "# EPOCHS = 10000\n",
        "# BATCH_SIZE = 128 \n",
        "# LEARN_R = 2.5e-1 for Tanh and LEARN_R = 1e-1 for ReLU\n",
        "\n",
        "# Recommended hyper-parameters for 3-D: \n",
        "# L  = [3, 16, 1] \n",
        "# EPOCHS = 10000\n",
        "# BATCH_SIZE = 128 \n",
        "# LEARN_R = 5e-2 for ReLU and LEARN_R = 1e-1 for Tanh\n",
        "\n",
        "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
        "## of the activation functions."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdqaqYBVFtIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize training hyperparameters\n",
        "EPOCHS = 20000\n",
        "BATCH_SIZE = 128 \n",
        "LEARN_R = 1e-2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "5kfg76GMFtIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "    # Mini-batch indexes\n",
        "    idx = np.random.choice(x_train.shape[0], size=BATCH_SIZE)\n",
        "    # Forward pass\n",
        "    aff, units = forward_pass(x_train[idx,:], NN, ACT_F)\n",
        "    # Backward pass\n",
        "    g_b, g_w = backward_pass(x_train[idx,:], y_train[idx], units[-1], units, aff, NN, ACT_F, squared_error)\n",
        "    \n",
        "    # Stochastic gradient descent\n",
        "    for l in range(len(g_b)):\n",
        "        NN[0][l] -= LEARN_R*g_w[l]\n",
        "        NN[1][l] -= LEARN_R*g_b[l]\n",
        "        \n",
        "    # Training loss\n",
        "    _, units = forward_pass(x_train, NN, ACT_F)\n",
        "    # Estimate loss function\n",
        "    #print(np.max(squared_error(y_train, units[-1])))\n",
        "    train_loss.append(np.mean(squared_error(y_train, np.squeeze(units[-1]))))\n",
        "    \n",
        "    # Validation\n",
        "    # Forward pass\n",
        "    _, units = forward_pass(x_validation, NN, ACT_F)\n",
        "    # Estimate validation loss function\n",
        "    val_loss.append(np.mean(squared_error(y_validation, np.squeeze(units[-1]))))\n",
        "    \n",
        "    if e%500==0:\n",
        "        print(\"{:4d}\".format(e),\n",
        "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
        "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VetyRWFwFtIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(range(len(train_loss)), train_loss);\n",
        "plt.plot(range(len(val_loss)), val_loss);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OgmIrM9FtIb",
        "colab_type": "text"
      },
      "source": [
        "# Testing\n",
        "\n",
        "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmNi7S-vFtIc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, units = forward_pass(x_test, NN, ACT_F)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mmJOTSEFtIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(y_test, units[-1]);\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='k');\n",
        "plt.xlabel(\"y\");\n",
        "plt.ylabel(\"$\\hat{y}$\");\n",
        "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
        "plt.grid(True);\n",
        "plt.axis('equal');\n",
        "plt.tight_layout();\n",
        "\n",
        "print(\"Test loss:  {:4.3f}\".format(np.mean(squared_error(y_test, np.squeeze(units[-1])))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODi0WlmQFtIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if D1:\n",
        "    plt.scatter(x_train[:,0], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test[:,0], units[-1], label=\"test prediction\");\n",
        "    plt.scatter(x_test[:,0], y_test, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");\n",
        "else:\n",
        "    plt.scatter(x_train[:,1], y_train, label=\"train data\");\n",
        "    plt.scatter(x_test[:,1], units[-1], label=\"test data prediction\");\n",
        "    plt.scatter(x_test[:,1], y_test, label=\"test data\");\n",
        "    plt.legend();\n",
        "    plt.xlabel(\"x\");\n",
        "    plt.ylabel(\"y\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBAmjsAFtIk",
        "colab_type": "text"
      },
      "source": [
        "## Exercise g) Show overfitting, underfitting and just right fitting\n",
        "\n",
        "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
        "\n",
        "See also if you can get a good compromise which leads to a low validation loss. \n",
        "\n",
        "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
        "\n",
        "_Insert written answer here._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tQZCn2dxFtIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Insert your code for getting overfitting, underfitting and just right fitting"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPZP-eTFtIo",
        "colab_type": "text"
      },
      "source": [
        "# Next steps - classification\n",
        "\n",
        "It is straight forward to extend what we have done to classification. \n",
        "\n",
        "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
        "\n",
        "Next week we will see how to perform classification in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsVPul3QFtIo",
        "colab_type": "text"
      },
      "source": [
        "## Exercise h) optional - Implement backpropagation for classification\n",
        "\n",
        "Should be possible with very few lines of code. :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "oC8QrI2tFtIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just add code."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}