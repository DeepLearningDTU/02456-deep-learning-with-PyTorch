{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits\n",
    "\n",
    "Originally created for a previous version of the [02456-deep-learning](https://github.com/DeepLearningDTU/02456-deep-learning) course material, but [converted to PyTorch](https://github.com/pytorch/tutorials).\n",
    "See repos for credits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependancies and supporting functions\n",
    "Loading dependancies and supporting functions by running the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cuda semantics in PyTorch\n",
    "\n",
    "> **NB:** If at possible you shuold run this notebooks using a GPU, i.e. use nvidia-docker and run the gpu image (training time is ~ 30 minutes on a new-ish laptop using the CPU).\n",
    "> If you don't have a GPU, and training takes to long consider reducing the dataset size of the dataset.\n",
    "\n",
    "When we train larger models it becomes infeasible to run on CPUs.\n",
    "Thankfully, GPU support is straightforward in PyTorch.\n",
    "\n",
    "See the [docs on cuda functionality](http://pytorch.org/docs/master/cuda.html) for an overview of useful functions for use with cuda.\n",
    "For more examples on cuda semantics see [the notes here](http://pytorch.org/docs/master/notes/cuda.html#cuda-semantics).\n",
    "\n",
    "\n",
    "## Transfer tensor to GPU\n",
    "\n",
    "Tensors can be transferred to the GPU by applying the `.cuda()` method to the \n",
    "[`torch.Tensor`s](http://pytorch.org/docs/master/tensors.html#torch-tensor) and \n",
    "[`torch.autograd.Variable`s](http://pytorch.org/docs/master/autograd.html#torch.autograd.Variable), e.g. `inputs.cuda()`.\n",
    "\n",
    "## Transfer network to GPU\n",
    "\n",
    "Just like how you transfer a Tensor on to the GPU, you transfer the entire network onto the GPU.\n",
    "This will recursively go over all modules and convert their parameters and buffers to CUDA tensors:\n",
    "\n",
    "```\n",
    "net.cuda()\n",
    "\n",
    "```    \n",
    "\n",
    "Remember that you will have to send the inputs and targets at every step to the GPU too:\n",
    "\n",
    "```\n",
    "inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "outputs = net(inputs)\n",
    "loss = criterion(outputs, labels)\n",
    "```\n",
    "\n",
    "## Use the cuda tensors / variables on CPU\n",
    "\n",
    "When tensors, modules, etc. are on a GPU they are no longer regular `torch.Tensor`s, but `torch.cuda.Tensor`s (see [torch's tensor overview](http://pytorch.org/docs/master/tensors.html#torch-tensor)).\n",
    "To be able to work with them on the host's memory, you need to fetch them from the GPU first.\n",
    "\n",
    "Given a cuda tensor or variable `ct` you can move it to host memory with `ct.cpu()`.\n",
    "If for instance you want to convert the tensor in a variable to numpy, you call `ct.cpu().data.numpy()`.\n",
    "If it was not on the GPU, we would not need the `.cpu()` call.\n",
    "\n",
    "## Setting up GPU in Colab\n",
    "\n",
    "In Colab, you will get 12 hours of execution time but the session will be disconnected if you are idle for more than 60 minutes. It means that for every 12 hours Disk, RAM, CPU Cache and the Data that is on our allocated virtual machine will get erased.\n",
    "\n",
    "To enable GPU hardware accelerator, just go to **Runtime -> Change runtime type -> Hardware accelerator -> GPU**\n",
    "\n",
    "## Multiple GPUs\n",
    "\n",
    "When you need to run a model on multiple GPUs you can use either \n",
    "[torch.nn.DataParallel](http://pytorch.org/docs/master/nn.html#torch.nn.DataParallel) or \n",
    "[torch.nn.parallel.DistributedDataParallel](http://pytorch.org/docs/master/nn.html#torch.nn.parallel.DistributedDataParallel).\n",
    "Depending on whether the GPUs are on multiple machines and how many CPUs you can utilize for handling the training.\n",
    "If you have more than two GPUs DitributedDataParallel is usually the better choice.\n",
    "\n",
    "It is straightforward to use this. Simply wrap the model as follows\n",
    "\n",
    "```\n",
    "model = Net() # define the model as always\n",
    "net = torch.nn.DataParallel(model, device_ids=None) # tell pytorch to use all available cuda devices\n",
    "output = net(input_var) # run on all devices\n",
    "```\n",
    "\n",
    "Check out [the data parallel example](http://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html) if you want to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Fun with convolutional networks\n",
    "## Get the data\n",
    "\n",
    "For this exercise we will use a modified version of MNIST - **MNIST Cluttered**.\n",
    "In the data the each mnist digit (20x20 pixels) has been placed randomly in a 60x60 canvas.\n",
    "To make the task harder each canvas has then been cluttered with small pieces of digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f mnist_cluttered_60x60_6distortions.npz ]; then wget -N https://www.dropbox.com/s/rvvo1vtjjrryr7e/mnist_cluttered_60x60_6distortions.npz; else echo \"mnist_cluttered_60x60_6distortions.npz already downloaded\"; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Transformer Networks\n",
    "\n",
    "In this task it is helpfull for a network if it can focus only on the digit and ignore the rest.\n",
    "The ``TransformerLayer`` lets us do this.\n",
    "The transformer layer learns an affine transformation which lets the network zoom, rotate and skew.\n",
    "If you are interested you should [read the paper](https://arxiv.org/abs/1506.02025), but the main idea is that you can let a small convolutional network determine the the parameters of the affine transformation.\n",
    "You then apply the affine transformation to the input data.\n",
    "Usually this also involves downsampling which forces the model to zoom in on the relevant parts of the data.\n",
    "After the affine transformation we can use a larger conv net to do the classification. \n",
    "This is possible because you can backprop through an affine transformation if you use bilinear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 15\n",
    "BATCH_SIZE = 256\n",
    "DIM = 60\n",
    "NUM_CLASSES = 10\n",
    "mnist_cluttered = \"mnist_cluttered_60x60_6distortions.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = np.load(mnist_cluttered)\n",
    "    X_train, y_train = data['x_train'], np.argmax(data['y_train'], axis=-1)\n",
    "    X_valid, y_valid = data['x_valid'], np.argmax(data['y_valid'], axis=-1)\n",
    "    X_test, y_test = data['x_test'], np.argmax(data['y_test'], axis=-1)\n",
    "\n",
    "    # reshape for convolutions\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, DIM, DIM))\n",
    "    X_valid = X_valid.reshape((X_valid.shape[0], 1, DIM, DIM))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, DIM, DIM))\n",
    "    \n",
    "    print(\"Train samples:\", X_train.shape)\n",
    "    print(\"Validation samples:\", X_valid.shape)\n",
    "    print(\"Test samples:\", X_test.shape)\n",
    "\n",
    "    return dict(\n",
    "        X_train=np.asarray(X_train, dtype='float32'),\n",
    "        y_train=y_train.astype('int32'),\n",
    "        X_valid=np.asarray(X_valid, dtype='float32'),\n",
    "        y_valid=y_valid.astype('int32'),\n",
    "        X_test=np.asarray(X_test, dtype='float32'),\n",
    "        y_test=y_test.astype('int32'),\n",
    "        num_examples_train=X_train.shape[0],\n",
    "        num_examples_valid=X_valid.shape[0],\n",
    "        num_examples_test=X_test.shape[0],\n",
    "        input_height=X_train.shape[2],\n",
    "        input_width=X_train.shape[3],\n",
    "        output_dim=10,)\n",
    "data = load_data()\n",
    "\n",
    "idx = 0\n",
    "canvas = np.zeros((DIM*NUM_CLASSES, NUM_CLASSES*DIM))\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        canvas[i*DIM:(i+1)*DIM, j*DIM:(j+1)*DIM] = data['X_train'][idx].reshape((DIM, DIM))\n",
    "        idx += 1\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(canvas, cmap='gray')\n",
    "plt.title('Cluttered handwritten digits')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "We use a model where the localization network is a two layer convolution network which operates directly on the image input. The output from the localization network is a 6 dimensional vector specifying the parameters in the affine transformation.\n",
    "\n",
    "We set up the transformer layer to initially do the identity transform, similarly to [1]. If the output from the localization networks is [t1, t2, t3, t4, t5, t6] then t1 and t5 determines zoom, t2 and t4 determines skewness, and t3 and t6 move the center position. By setting the initial values of the bias vector to \n",
    "\n",
    "```\n",
    "|1, 0, 0|\n",
    "|0, 1, 0|\n",
    "```\n",
    "and the final W of the localization network to all zeros we ensure that in the beginning of training the network works as a pooling layer. \n",
    "\n",
    "The output of the localization layer feeds into the transformer layer which applies the transformation to the image input. In our setup the transformer layer downsamples the input by a factor 3.\n",
    "\n",
    "Finally a 2 layer convolution layer and 2 fully connected layers calculates the output probabilities.\n",
    "\n",
    "\n",
    "### The model\n",
    "```\n",
    "Input -> localization_network -> TransformerLayer -> output_network -> predictions\n",
    "   |                                |\n",
    "   >--------------------------------^\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_channels, input_height, input_width, num_classes, num_zoom=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_height = input_height\n",
    "        self.input_width = input_width\n",
    "        self.num_classes = num_classes\n",
    "        self.num_zoom = num_zoom\n",
    "        \n",
    "        # Spatial transformer localization-network\n",
    "        # nn.Sequential http://pytorch.org/docs/master/nn.html#torch.nn.Sequential\n",
    "        #   A sequential container. \n",
    "        #   Modules will be added to it in the order they are passed in the constructor.\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_channels,\n",
    "                      out_channels=8, \n",
    "                      kernel_size=7, \n",
    "                      padding=3),\n",
    "            nn.MaxPool2d(kernel_size=2, \n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=8, \n",
    "                      out_channels=10, \n",
    "                      kernel_size=5, \n",
    "                      padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix that we use \n",
    "        # to make the bilinear interpolation for the spatial transformer\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(in_features=10 * input_height//4 * input_width//4, \n",
    "                      out_features=32,\n",
    "                      bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=32, \n",
    "                      out_features=3 * 2,\n",
    "                      bias=True)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        # see the article for a definition and explanation for this\n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "        \n",
    "        # The classification network based on the transformed (cropped) image\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels,\n",
    "                               out_channels=16,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, \n",
    "                               out_channels=32,\n",
    "                               kernel_size=5,\n",
    "                               padding=2)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        \n",
    "        # fully connected output layers\n",
    "        self.fc1_features = 32 * input_height//num_zoom//4 * input_width//num_zoom//4\n",
    "        self.fc1 = nn.Linear(in_features=self.fc1_features, \n",
    "                             out_features=50)\n",
    "        self.fc2 = nn.Linear(in_features=50,\n",
    "                             out_features=num_classes)\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        \"\"\" Spatial Transformer Network \"\"\"\n",
    "        # creates distributed embeddings of the image with the location network.\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(-1, 10 * self.input_height//4 * self.input_width//4)\n",
    "        # project from distributed embeddings to bilinear interpolation space\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        \n",
    "        # define the output size of the cropped tensor\n",
    "        # notice that we divide the height and width with the amount of zoom\n",
    "        output_size = torch.Size((x.size()[0],\n",
    "                                  x.size()[1], \n",
    "                                  x.size()[2]//self.num_zoom,\n",
    "                                  x.size()[3]//self.num_zoom))\n",
    "        # magic pytorch functions that are used for transformer networks\n",
    "        grid = F.affine_grid(theta, output_size, align_corners=True) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.affine_grid\n",
    "        x = F.grid_sample(x, grid, align_corners=True) # http://pytorch.org/docs/master/nn.html#torch.nn.functional.grid_sample\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # transform the input\n",
    "        x = self.stn(x)\n",
    "        # save transformation\n",
    "        l_trans1 = Variable(x.data)\n",
    "\n",
    "        # Perform the usual forward pass\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, self.fc1_features)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # note use of Functional.dropout, where training must be explicitly defined (default: False)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        # return output and batch of bilinear interpolated images\n",
    "        return F.log_softmax(x, dim=1), l_trans1\n",
    "\n",
    "\n",
    "net = Net(1, DIM, DIM, NUM_CLASSES)\n",
    "if torch.cuda.is_available():\n",
    "    print('##converting network to cuda-enabled')\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test forward pass on dummy data\n",
    "x = np.random.normal(0,1, (45, 1, 60, 60)).astype('float32')\n",
    "x = Variable(torch.from_numpy(x))\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "output = net(x)\n",
    "print([x.size() for x in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Training convnets on CPU is painfully slow.\n",
    "After 10 epochs you should see that model starts to zoom in on the digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(X, y,):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    costs = []\n",
    "    correct = 0\n",
    "    net.train()\n",
    "    for i in range(num_batches):\n",
    "        if i % 10 == 0:\n",
    "            print(\"{}, \".format(i), end='')\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_tr = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "        y_batch_tr = get_variable(Variable(torch.from_numpy(y[idx]).long()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = net(X_batch_tr)\n",
    "        batch_loss = criterion(output, y_batch_tr)\n",
    "        \n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        costs.append(get_numpy(batch_loss))\n",
    "        preds = np.argmax(get_numpy(output), axis=-1)\n",
    "        correct += np.sum(get_numpy(y_batch_tr) == preds)\n",
    "    print()\n",
    "    return np.mean(costs), correct / float(num_samples)\n",
    "\n",
    "def eval_epoch(X, y):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / float(BATCH_SIZE)))\n",
    "    pred_list = []\n",
    "    transform_list = []\n",
    "    net.eval()\n",
    "    for i in range(num_batches):\n",
    "        if i % 10 == 0:\n",
    "            print(\"{}, \".format(i), end='')\n",
    "        idx = range(i*BATCH_SIZE, np.minimum((i+1)*BATCH_SIZE, num_samples))\n",
    "        X_batch_val = get_variable(Variable(torch.from_numpy(X[idx])))\n",
    "        output, transformation = net(X_batch_val)\n",
    "        pred_list.append(get_numpy(output))\n",
    "        transform_list.append(get_numpy(transformation))\n",
    "    transform_eval = np.concatenate(transform_list, axis=0)\n",
    "    preds = np.concatenate(pred_list, axis=0)\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    acc = np.mean(preds == y)\n",
    "    print()\n",
    "    return acc, transform_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note you can interrupt training to visualize progress along the way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_accs, train_accs, test_accs = [], [], []\n",
    "\n",
    "while n < NUM_EPOCHS:\n",
    "    n += 1\n",
    "    try:\n",
    "        print(\"Epoch %d:\" % n)\n",
    "        print('train: ')\n",
    "        train_cost, train_acc = train_epoch(data['X_train'], data['y_train'])\n",
    "        print('valid ')\n",
    "        valid_acc, valid_trainsform = eval_epoch(data['X_valid'], data['y_valid'])\n",
    "        print('test ')\n",
    "        test_acc, test_transform = eval_epoch(data['X_test'], data['y_test'])\n",
    "        valid_accs += [valid_acc]\n",
    "        test_accs += [test_acc]\n",
    "        train_accs += [train_acc]\n",
    "\n",
    "        print(\"train cost {0:.2}, train acc {1:.2}, val acc {2:.2}, test acc {3:.2}\".format(\n",
    "                train_cost, train_acc, valid_acc, test_acc))\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nKeyboardInterrupt')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot errors and zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 9))\n",
    "plt.plot(1 - np.array(train_accs), label='Training Error')\n",
    "plt.plot(1 - np.array(valid_accs), label='Validation Error')\n",
    "plt.legend(fontsize=20)\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('Error', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 14))\n",
    "for i in range(3):\n",
    "    plt.subplot(321 + i * 2)\n",
    "    plt.imshow(data['X_test'][i].reshape(DIM, DIM), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Original 60x60', fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(322+i*2)\n",
    "    plt.imshow(test_transform[i].reshape(DIM//3, DIM//3), cmap='gray', interpolation='none')\n",
    "    if i == 0:\n",
    "        plt.title('Transformed 20x20', fontsize=20)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few pointers for image classification\n",
    "\n",
    "If you want do image classification, using a pretrained model is often a good choice, especially if you have limited amounts of labelled data.\n",
    "PyTorch has a \n",
    "[guide for using their current state-of-the-art pretrained model](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) in their \n",
    "[model repository](https://github.com/pytorch/vision/tree/master/torchvision/models).\n",
    "Torch7 has [an interesting blog post about residual nets](http://torch.ch/blog/2016/02/04/resnets.html), an architecture that was very popular and influential a while back."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
